%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proofs for CPT-value estimator}
\label{appendix:cpt-est}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\holder continuous weights}
\label{sec:holder-proofs}
For proving Proposition \ref{prop:holder-asymptotic} and \ref{prop:sample-complexity-discrete}, we require Hoeffding's inequality, which is given below.
\begin{lemma}
Let $Y_1,...Y_n$ be independent random variables satisfying $\Prob{a\leq Y_i \leq b}= 1,$ for each $i$, where $a<b.
$ Then for $t>0$,
$$\Prob{\left|\sum_{i=1}^n Y_i -\sum_{i=1}^n E(Y_i)\right| \geq nt } \leq 2\exp{\{-2nt^2 /(b-a)^2\}}. $$
\end{lemma}

\begin{proposition}
\label{prop:Holder-cpt-finite}
Under (A1'), the CPT-value $\C(X)$ as defined by \eqref{eq:cpt-general} is finite. 
\end{proposition}
\begin{proof}

H\"{o}lder continuity of $w^+$ together with the fact that $w^+(0)=0$ imply that 
\begin{align*}
&\int_0^{\infty} w^+\left(\Prob{u^{+}(X)>t}\right) dz 
\le H \int_0^{\infty} \mathbb{P}^{\alpha} \left(u^+(X)>z\right) dz\\
&\le H \int_0^{\infty} \mathbb{P}^{\gamma} \left(u^+(X)>z\right) dz 
<\infty.
\end{align*}
The second inequality is valid since $\Prob{u^+(X)>z} \leq 1$. The claim follows for the first integral in \eqref{eq:cpt-general} and the finiteness of the second integral in \eqref{eq:cpt-general} can be argued in an analogous fashion.
\end{proof}

%%%%
\begin{proposition}
\label{prop:holder-quantile}
Assume (A1'). Let $\xi^+_{\frac{i}{n}}$ and $\xi^-_{\frac{i}{n}}$ denote the $\frac{i}{n}$th quantile of $u^+(X)$ and $u^-(X)$, respectively. Then, we have  \todoc{Fix the ugly parenthesis! Also, why do we suddenly number all equations as if they all needed a number??}
\begin{align}
\label{eq:simple-estimation}
\begin{split}
\lim_{n \rightarrow \infty} \sum_0^{n-1} \xi^+_{\frac{i}{n}} \left(w^+\left(\frac{n-i}{n}\right)- w^+\left(\frac{n-i-1}{n}\right) \right) \\
= \int_0^{+\infty} w^+\left(\Prob{u^+(X)>z}\right) dz < +\infty,
\\
\lim_{n \rightarrow \infty} \sum_0^{n-1} \xi^-_{\frac{i}{n}} \left(w^-\left(\frac{n-i}{n}\right)- w^-\left(\frac{n-i-1}{n}\right) \right) \\
= \int_0^{+\infty} w^-\left(\Prob{u^-(X)>z}\right) dz < +\infty
\end{split}
\end{align}
\end{proposition}

\todoj[inline]{Fix the upper limit..I think it should run up to $n$}

\begin{proof}
We shall focus on proving the first part of equation \eqref{eq:simple-estimation}. Consider the following linear combination of simple functions: 
\begin{align}
\sum_{i=0}^{n-1} w^+ \left(\frac{i}{n}\right) 
I_{\left[\xi^+_\frac{n-i-1}{n}, \xi^+_\frac{n-i}{n}\right]}(t),
\label{eq:simplew}
\end{align}
which will converge almost everywhere to the function $w^+\left(\Prob{u^{+}(X)>t}\right)$ in the interval $[0, +\infty)$, and also notice that, for all $t \in [0,+\infty)$, we have
\begin{align*}
\sum_{i=0}^{n-1} w^+ \left(\frac{i}{n}\right) 
I_{\left[\xi^+_\frac{n-i-1}{n}, \xi^+_\frac{n-i}{n}\right]}(t)
<
w^+\left(\Prob{u^{+}(X)>t}\right).
\end{align*}

The integral of \eqref{eq:simplew} can be simplified as follows:
\begin{align*}
& \int_0^{+\infty} \sum_{i=0}^{n-1} w^+ \left(\frac{i}{n}\right) 
\cdot I_{\left[\xi^+_\frac{n-i-1}{n}, \xi^+_\frac{n-i}{n}\right]}(t) \\
 & = \sum_{i=0}^{n-1} w^+\left(\frac{i}{n}\right) \left(\xi^+_{\frac{n-i}{n}} -
\xi^+_{\frac{n-i-1}{n}}\right) \\ & = \sum_{i=0}^{n-1} \xi^+_{\frac{i}{n}} \left(w^+\left(\frac{n-i}{n}\right)-
    w^+\left(\frac{n-i-1}{n}\right)\right).
\end{align*}
The H\"{o}lder continuity property assures the fact that 
$\lim_{n \rightarrow \infty}  | w^+(\frac{n-i}{n})- w^+(\frac{n-i-1}{n})| =0$, and the limit in \eqref{eq:simple-estimation} holds through a typical application of the dominated convergence theorem.

The second part of \eqref{eq:simple-estimation} can be justified in a similar fashion.
\end{proof} 
%%%

%%%%%%%%%\xi^+
\subsection*{Proof of Proposition \ref{prop:holder-asymptotic}}

\begin{proof}
Without loss of generality, assume that \holder constant $H$ of the weight functions $w^+$ and $w^-$ are both  $1$.
We first prove  that 
$$\overline \C^{+}_n
\rightarrow
\C^{+}(X)
 \text{   a.s. as } n\rightarrow \infty.$$
The statement above is equivalent to the following: \todoc{Don't start sentences with ''and'' or ''or''.}
\begin{align}
&\lim_{n\rightarrow +\infty} \sum_{i=1}^{n-1} u^+\left(X_{[i]}\right) \left(w^+\left(\frac{n-i+1}{n}\right)- w^+\left(\frac{n-i}{n}\right)\right)\\
&\xrightarrow{n \rightarrow\infty} \int_0^{+\infty} w^+\left(P\left(U>t\right)\right) dt , \text{w.p. } 1
\label{eq:claim11}
\end{align}


The main part of the proof is concentrated on finding an upper bound of the probability
\begin{align*}
\mathbb{P} \left( \left| \sum_{i=1}^{n-1} u^+\left(X_{[i]}\right) \cdot \left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right) \right.\right.\\
\quad\left.\left. -
\sum_{i=1}^{n-1} \xi^+_{\frac{i}{n}} \cdot \left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right) \right| >
\epsilon\right),
\end{align*}
for any given $\epsilon>0$.
Observe that
\begin{align*}
& \mathbb{P} \left( \left| \sum_{i=1}^{n-1} u^+\left(X_{[i]}\right) \cdot \left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right) \right.\right. -\\
&\qquad\left.\left.\sum_{i=1}^{n-1} \xi^+_{\frac{i}{n}} \cdot \left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right) \right| >
\epsilon\right) \\ 
%%%%%%%%%%%%%%%%%%%%%%%%
& \leq \mathbb{P}\left( \bigcup _{i=1}^{n-1} \left\{ \left| u^+\left(X_{[i]}\right) \cdot \left(w^+\left(\frac{n-i}{n}\right) -
w^+\left(\frac{n-i-1}{n}\right)\right)\right.\right.\right. \\
&\qquad\left.\left.\left.- \xi^+_{\frac{i}{n}} \cdot \left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right)
\right| > \frac{\epsilon}{n} \right\}\right) \\ 
%%%%%%%%%%%
& \leq \sum _{i=1}^{n-1} \mathbb{P}\left( \left| u^+\left(X_{[i]}\right) 
\left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right)\right.\right. \\
&\qquad\left.\left.- \xi^+_{\frac{i}{n}} \cdot \left(w^+\left(\frac{n-i}{n}\right) -
w^+\left(\frac{n-i-1}{n}\right)\right) \right| > \frac{\epsilon}{n}\right) \\ 
%%%%%%%%%%%
& = \sum _{i=1}^{n-1} \mathbb{P}\left( \left| \left( u^+\left(X_{[i]}\right) -
\xi^+_{\frac{i}{n}}\right)\right.\right. \\
&\quad\qquad\left.\left.\times\left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right) \right| > \frac{\epsilon}{n}\right)
\\ 
& \leq \sum _{i=1}^{n-1} \Prob{ \left| \left( u^+\left(X_{[i]}\right) - \xi^+_{\frac{i}{n}}\right) \left(\frac{1}{n}\right)^{\alpha}
\right| > \frac{\epsilon}{n}} \stepcounter{equation}\tag{\theequation}\label{eq:bd02}\\ 
& = \sum _{i=1}^{n-1} \Prob{ \left| \left( u^+\left(X_{[i]}\right) - \xi^+_{\frac{i}{n}}\right)
\right| > \frac{\epsilon}{\cdot n^{1-\alpha}}}.\stepcounter{equation}\tag{\theequation}\label{eq:bd12}
\end{align*}
In the above, \eqref{eq:bd02} follows from the fact that $w^+$ is \holder with constant $1$.

Now we find the upper bound of the probability of a single item in the sum above, i.e.,
\begin{align*}
& \Prob{ \left | u^+\left(X_{[i]}\right) - \xi^+_{\frac{i}{n}} \right | > \frac {\epsilon} {n^{\left(1-\alpha\right)}}} \\ 
& = \Prob{
    u^+\left(X_{[i]}\right) - \xi^+_{\frac{i}{n}} > \frac {\epsilon} {n^{\left(1-\alpha\right)}}}\\
	&	+ \Prob{ u^+\left(X_{[i]}\right) -
    \xi^+_{\frac{i}{n}} < - \frac {\epsilon} {n^{\left(1-\alpha\right)}}}.
\end{align*} 

We focus on the term 
$
\Prob{u^+(X_{[i]}) - \xi^+_{\frac{i}{n}} > \frac {\epsilon}{\nalpha}}
$.
Let $$W_t = I_{\left(u^+(X_t) > \xi^+_{\frac{i}{n}} + \frac{\epsilon}{n^{(1-\alpha)}}\right)}, t=1, \ldots,n.$$ Using the fact that a probability distribution function is non-decreasing, we obtain 
\begin{align*}
& \Prob { u^+(X_{[i]}) - \xi^+_{\frac{i}{n}} > \frac {\epsilon}{\nalpha}}  \\
& = \Prob { \sum _{t=1}^{n} W_t > n\cdot\left(1-\frac{i}{n^{(1-\alpha)}}\right)} \\ 
& = \mathbb{P} \left( \sum _{t=1}^{n} W_t - n \cdot \left[1-F^{+}\left(\xi^+_{\frac{i}{n}}
+\frac{\epsilon}{n^{(1-\alpha)}}\right)\right] \right. \\
&\left.\quad\qquad> n \cdot \left[F^{+}\left(\xi^+_{\frac{i}{n}} +\frac{\epsilon}{n^{(1-\alpha)}}\right)
- \frac{i}{n}\right]\right).
\end{align*}

Using the fact that 
$E W_t = 1-F^{+}\left(\xi^+_{\frac{i}{n}} +\frac{\epsilon}{n^{(1-\alpha)}}\right)$ in conjunction with Hoeffding's inequality, we obtain
\todoj{Shouldnt Hoeffding have $\le$ on RHS instead of $<$}
\todoj{change to $n(1-i/n)$ here}
\begin{align}
&\mathbb{P} \left( \sum _{i=1}^{n} W_t - n \cdot \left[1-F^{+}\left(\xi^+_{\frac{i}{n}} +\frac{\epsilon}{n^{(1-\alpha) }  } \right) \right] \right.\\
&\left.\qquad> n
\cdot \left[F^{+}\left(\xi^+_{\frac{i}{n}} +\frac{\epsilon}{n^{(1-\alpha)} } \right) - \frac{i}{n}\right]\right) < e^{-2n\cdot
\delta^{'}_t},
\end{align}
where $\delta^{'}_i = F^{+}\left(\xi^+_{\frac{i}{n}} +\frac{\epsilon} {n^{(1-\alpha)} }\right) - \frac{i}{n}$. Since 
$F^{+}$ is Lipschitz, we have that $ \delta^{'}_i \leq L^{+} \cdot \left(\frac{\epsilon}{\nalpha}\right)$.
Hence, we obtain
\begin{align}
\Prob{ u^+(X_{[i]}) - \xi^+_{\frac{i}{n}} > \frac {\epsilon}{\nalpha}} &< e^{-2n\cdot L^{+}
\frac{\epsilon}{\nalpha} } \nonumber\\
&= e^{-2n^\alpha \cdot L ^{+} \epsilon}
\label{eq:a12}
\end{align}
In a  similar fashion, one can show that 
\begin{align}
\Prob{ u^+(X_{[i]}) -\xi^+_{\frac{i}{n}} < -\frac {\epsilon} {\nalpha}} \leq e^{-2n^\alpha \cdot L^{+}  \epsilon}.
\label{eq:a23}
\end{align}
Combining \eqref{eq:a12} and \eqref{eq:a23},  we obtain 
%\todoc{Is not $\mathbb{N} \cap (0,1) =\emptyset$???}
\begin{align*}
\Prob{ \left| u^+(X_{[i]}) -\xi^+_{\frac{i}{n}} \right| < -\frac {\epsilon} {\nalpha}} \leq 2\cdot
e^{-2n^\alpha \cdot L^{+} \epsilon} , 
\end{align*}
Plugging the above in \eqref{eq:bd12}, we obtain
\begin{align}
&
\mathbb{P} \left( \left| \sum_{i=1}^{n-1} u^+\left(X_{[i]}\right) \cdot \left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right) \right.\right. \nonumber\\
&\left.\left.\quad -
\sum_{i=1}^{n-1} \xi^+_{\frac{i}{n}} \cdot \left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right) \right| >
\epsilon\right) \nonumber\\
&\leq 2n\cdot e^{-2n^\alpha \cdot L^{+}}.\label{eq:holder-sample-complexity-extract}
\end{align}

Notice that $\sum_{n=1}^{+\infty}  2n \cdot e^{-2n^{\alpha}\cdot L^{+} \epsilon}< \infty$ since the sequence 
$2n \cdot e^{-2n^{\alpha}\cdot L^{+}}$ will decrease more rapidly than the sequence
$\frac{1}{n^k}$, $\forall k>1$.

By applying the Borel Cantelli lemma,  $\forall \epsilon >0$, we have that
\begin{align*}
&\mathbb{P} \left( \left| \sum_{i=1}^{n-1} u^+\left(X_{[i]}\right) \cdot \left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right)\right.\right. \\
&\left.\left.-
\sum_{i=1}^{n-1} \xi^+_{\frac{i}{n}} \cdot \left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right) \right| >
\epsilon , i.o.\right) \\
&=0, 
\end{align*}
which implies 
\begin{align*}
&\sum_{i=1}^{n-1} u^+\left(X_{[i]}\right) \cdot \left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right) \\
&- \sum_{i=1}^{n-1}
\xi^+_{\frac{i}{n}} \cdot \left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right)\\ &\xrightarrow{n \rightarrow
+\infty} 0 \text{   w.p } 1 ,
\end{align*}
which proves \eqref{eq:claim11}. 

The proof of 
$\C_{n}^{-} \rightarrow \C^{-}(X)$ follows in a similar manner as above by replacing $u^+(X_{[i]})$ by $u^-(X_{[n-i]})$, after observing that $u^{-}$ is decreasing, which in turn implies that
$u^-(X_{[n-i]})$ is an estimate of the quantile $\xi^{-}_{\frac{i}{n}}$.
\end{proof}

\subsection*{Proof of Proposition \ref{prop:holder-dkw}}
For proving Proposition \ref{prop:holder-dkw}, we require the following well-known inequality that provide a finite-time bound on the distance between empirical distribution and the true distribution:
\begin{lemma}{\textbf{\textit{(Dvoretzky-Kiefer-Wolfowitz (DKW) inequality)}}}\\
Let ${\hat F_n}(u)=\frac{1}{n} \sum_{i=1}^n I_{\left[(u(X_i)) \leq u\right]}$ denote the empirical distribution of a r.v. $U$, with $u(X_1),\ldots,u(X_n)$ being sampled from the r.v $u(X)$.
The, for any $n$ and $\epsilon>0$, we have
$$
\Prob{\sup_{x\in \mathbb{R}}|\hat{F}_n(x)-F(x)|>\epsilon } \leq 2 e^{-2n\epsilon^2}.
$$
\end{lemma}

The reader is referred to Chapter 2 of \cite{wasserman2006} for detailed description of empirical distributions in general and DKW inequality in particular.

\begin{proof}
We prove the $w^+$ part, and the $w^-$ part follows in a similar fashion.
Since $u^+(X)$ is bounded above by $M$ and $w^+$ is H\"{o}lder-continuous, we have
\begin{align*}
&\left|\int_0^{\infty} w^+\left(\Prob{u^+(X)>t}\right) dt- \int_0^{\infty} w^+\left(1- {\hat F^+_n}(t)\right) dt\right| \\ = &
    \left|\int_0^M w^+\left(\Prob{u^+(X)>t}\right) dt- \int_0^M w^+\left(1- {\hat F^+_n}(t)\right) dt\right| \\
\leq& \left|\int_0^M H\cdot |\Prob{u^+(X)<t}-{\hat F^+_n}(t)|^\alpha dt\right|\\ \leq& HM\sup_{x\in
\mathbb{R}}\left|\Prob{u^+(X)<t}-{\hat F^+_n}(t)\right|^\alpha.
\end{align*}
Now, plugging in the DKW inequality, we obtain
\begin{align}
&\mathbb{P}\left(\left|\intinfinity w^+\left(\Prob{u^+(X)>t}\right) dt \right.\right.\nonumber\\
&\quad\left.\left.- \intinfinity w^+\left(1- {\hat F^+_n}(t)\right) dt\right|>\epsilon\right)
\nonumber\\
& \leq \Prob{HM\sup_{t\in \mathbb{R}} \left|(\Prob{u^+(X)<t}-{\hat F^+_n}(t)\right|^\alpha>\epsilon}\nonumber\\
&
\leq  e^{-n \frac{\epsilon ^{(2/\alpha)}} {2 H^2 M^2}}.\label{eq:dkw3}
\end{align}
The claim follows.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%Lipschitz-starts-here %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lipschitz continuous weights}
\label{sec:lipschitz-proofs}
\todoj[inline]{Can't we have a unified proof of \holder and Lipschitz cases?}
 Setting $\alpha=\gamma=1$ in the proof of Proposition \ref{prop:lipschitz}, it is easy to see that the CPT-value \eqref{eq:cpt-general} is finite. 

Next, in order to prove the asymptotic convergence claim in Proposition \ref{prop:lipschitz}, we require the dominated convergence theorem in its generalized form, which is provided below.
\begin{theorem}{\textbf{\textit{(Generalized Dominated Convergence theorem)}}}
Let $\{f_n\}_{n=1}^\infty$ be a sequence of measurable functions on $E$ that converge pointwise a.e. on a measurable space $E$ to $f$.  Suppose there is a sequence $\{g_n\}$ of integrable functions on $E$ that converge pointwise a.e. on $E$ to $g$ such that $|f_n| \leq g_n$ for all $n \in \mathbb{N}$.  
If $\lim\limits_{n \rightarrow \infty}$ $\int_E$ $g_n$ = $\int_E$ $g$, then $\lim\limits_{n \rightarrow \infty}$ $\int_E$ $f_n$ = $\int_E$ $f$.
\end{theorem}


\begin{proof}
This is a standard result that can be found in any textbook on measure theory. For instance, see Theorem 2.3.11 in \cite{athreya2006measure}.
\end{proof}


\subsection*{Proof of Proposition \ref{prop:lipschitz}: Asymptotic convergence}
\begin{proof}
Notice the the following equivalence: \todoc{The hats should be on $F$ only, as in $\hat{F}^+$ as opposed to $\hat{F^+}$.}
\begin{align*}
&\sum_{i=1}^{n} u^+\left(X_{[i]}\right) \left(w^+\left(\frac{n+1-i}{n}\right) - w^+\left(\frac{n-i}{n}\right)\right) \\
&=  \int_0^M w^+\left(1-\hat{F}^+_n\left(x\right)\right) dx, 
\end{align*}
and also,
\begin{align*}
&\sum_{i=1}^{n} u^-\left(X_{[i]}\right) \left(w^-\left(\frac{i}{n}\right) - w^-\left(\frac{i-1}{n}\right)\right)\\
& =  \int_0^M w^-\left(1-\hat{F}^-_n\left(x\right)\right) dx, 
\end{align*}
where $\hat{F}^+_n\left(x\right)$ and $\hat{F}^-_n\left(x\right)$ are the empirical distributions of $u^+\left(X\right)$
and $u^-\left(X\right)$, respectively.

Thus, the CPT estimator $\overline \C_n$ in Algorithm \ref{alg:holder-est} can be written equivalently as follows:
\begin{align}
\overline \C_n = \intinfinity w^+\left(1-{\hat F_n}^+\left(x\right)\right)  dx \!-\! \intinfinity w^-\left(1-{\hat F_n}^-\left(x\right)\right)  dx.
\label{eq:cpt-est-appendix}
\end{align}
We first prove the asymptotic convergence claim for the first integral in \eqref{eq:cpt-est-appendix}, i.e., we show
\begin{align}
\intinfinity w^+\left(1-{\hat F_n}^+\left(x\right)\right)  dx \rightarrow \intinfinity w^+\left(\Prob{u^+\left(X\right)>x}\right) dx.\label{eq:3}
\end{align} 

Since $w^+$ is Lipschitz continuous with, say, constant $L$, we have almost surely that
$w^{+}\left(1-\hat{F}_n\left(x\right)\right) \leq L \left(1-\hat{F}_n\left(x\right)\right)$,  
for all $n$ and 
 $w^{+}\left(\Prob{u^+\left(X\right)>x}\right) \leq L\cdot \left(\Prob{u^+\left(X\right)>x}\right)$, since $w^+\left(0\right)=0$.
 
% Notice that the empirical distribution function 
% ${\hat F_n}^+\left(x\right)$ 
% generates a Stieltjes measure which takes mass 
% $1/n$ on each of the sample points $u^+\left(X_{i}\right)$. 

We have
$$\intinfinity \left(\Prob{u^+\left(X\right)>x}\right)  dx = \EE{u^+\left(X\right)}$$
and

\begin{equation}
\intinfinity \left(1-{\hat F_n}^+\left(x\right)\right)  dx =\intinfinity \int_x^\infty d \hat{F}_n\left(t\right) dx.\label{eq:2}
\end{equation}
Since ${\hat F_n}^+\left(x\right)$ has bounded support on $\mathbb{R}$ $\forall n$, the integral in \eqref{eq:2} is finite.
Applying Fubini's theorem to the RHS of \eqref{eq:2}, we obtain
\begin{align*}
\intinfinity \int_x^\infty d \hat{F}_n\left(t\right) dx = \intinfinity \int_0^t dx d \hat{F}_n\left(t\right) \\
= \intinfinity t d\hat{F}_n\left(t\right) = \frac{1}{n} \sum_{i=1}^n u^+\left(X_{[i]}\right),
 \end{align*}
 where $u^+\left(X_{[i]}\right), i=1,\ldots,n$ denote the order statistics, i.e., $u^+\left(X_{[1]}\right) \le \ldots \le u^+\left(X_{[n]}\right)$.
 
Notice that 
\begin{align*}
\frac{1}{n}
\sum_{i=1}^n u^+\left(X_{[i]}\right)
=
\frac{1}{n}
\sum_{i=1}^n u^+\left(X_{i}\right)
\overset{a.s}\longrightarrow 
\EE{u^+\left(X\right)},
\end{align*}
From the foregoing,
\begin{align*}
\lim_{n\rightarrow \infty} \intinfinity L\left(1-\hat{F}_n\left(x\right)\right) dx\\
\overset{a.s} \longrightarrow
\intinfinity L \left(\Prob{u^+\left(X\right)>x}\right) dx.
\end{align*}
Hence, we obtain
\begin{align*}
\int_0^\infty w^{\left(+\right)}\left(1-\hat{F}_n\left(x\right)\right) dx\\
 \xrightarrow{a.s.} 
\int_0^\infty w^{\left(+\right)}\left(\Prob{u^+\left(X\right)\right)>x} dx.
\end{align*}
The claim in \eqref{eq:3} now follows by invoking the generalized dominated convergence theorem by setting $f_n = w^+(1-{\hat F_n}^+(x))$ and $g_n = L\cdot(1-\hat{F}_n(x))$, and noticing that $L\cdot(1-\hat{F}_n(x)) \xrightarrow{a.s.} L(\Prob{u^+(X)>x})$ uniformly $\forall x$. The latter fact is implied by the Glivenko-Cantelli theorem (cf. Chapter 2 of \cite{wasserman2006}).

Following similar arguments, it is easy to show that 
$$
\intinfinity w^-\left(1-{\hat F_n}^-(x)\right)  dx \rightarrow \intinfinity w^-\left(\Prob{u^-(X)>x}\right) dx.
$$
The final claim regarding the almost sure convergence of \\$\overline \C_n$ to $\C(X)$ now follows.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Proof of Proposition \ref{prop:lipschitz}: Sample complexity}

\begin{proof}
Since $u^+(X)$ is bounded above by $M$ and $w^+$ is Lipschitz with constant $L$, we have
\begin{align*}
&\left|\intinfinity w^+\left(\Prob{u^+(X)>x}\right) dx- \intinfinity w^+\left(1- \hat F_n^+(x)\right) dx\right|
\\
&=  \left|\int_0^M w^+\left(\Prob{u^+(X)>x}\right) - \int_0^M w^+(1- {\hat F_n}^+(x)) dx\right|
\\
&\leq
\left|\int_0^M L\cdot |\Prob{u^+(X)<x}-{\hat F_n}^+(x)| dx\right|\\
&\leq
LM\sup_{x\in \mathbb{R}}\left|\Prob{u^+(X)<x}-{\hat F_n}^+(x)\right|.
\end{align*}
Now, plugging in the DKW inequality, we obtain
\begin{align}
&
\mathbb{P}\left(\left|\intinfinity w^+\left(\Prob{u^+(X)>x}\right) dx\right.\right.\nonumber\\
&\quad\left.\left.- \intinfinity w^+\left(1- {\hat F_n}^+(x)\right) dx\right|>\epsilon/2\right)
\nonumber\\
&
\leq
 \Prob{LM\sup_{x\in \mathbb{R}} \left|(\Prob{u^+(X)<x}-{\hat F_n}^+(x)\right|>\epsilon/2} \nonumber\\
&\leq 2 e^{-n \frac{\epsilon^2}{2 L^2 M^2}}.\label{eq:dkw1}
\end{align}

Along similar lines, we obtain
\begin{align}
&
\mathbb{P}\left(\left|\intinfinity w^-\left(\Prob{u^-(X)>x}\right) dx \right.\right.\nonumber\\
&\quad\left.\left.- \intinfinity w^-\left(1- {\hat F_n}^-(x)\right) dx\right|>\epsilon/2\right)\nonumber\\
& \leq 2 e^{-n \frac{\epsilon^2}{2 L^2 M^2}}.\label{eq:dkw2}
\end{align}

Combining \eqref{eq:dkw1} and \eqref{eq:dkw2}, we obtain
\begin{align*}
&\Prob{|\overline \C_n - \C(X)|>\epsilon}\\ 
&\le \mathbb{P}\left(\left|\intinfinity w^+\left(\Prob{u^+(X)>x}\right) dx\right.\right.\\
&\quad\left.\left.- \intinfinity w^+\left(1- {\hat F_n}^+(x)\right) dx\right|>\epsilon/2\right) \\
&+ 
\mathbb{P}\left(\left|\intinfinity w^-\left(\Prob{u^-(X)>x}\right) dx\right.\right.\\
&\left.\left.\quad- \intinfinity w^-\left(1- {\hat F_n}^-(x)\right) dx\right|>\epsilon/2\right)\\
&\le 4 e^{-n \frac{\epsilon^2}{2 L^2 M^2}}.
\end{align*} 
And the claim follows. 
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%Lipschitz-ends-here %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proofs for discrete valued $X$}
\label{sec:proofs-discrete}
\input{proofs-discrete}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proofs for CPT-SPSA-G}
\label{appendix:1spsa}

To prove the main result in Theorem \ref{thm:1spsa-conv}, we first show, in the following lemma, that the gradient estimate using SPSA is only an order $O(\delta_n^2)$ term away from the true gradient. The proof differs from the corresponding claim for regular SPSA (see Lemma 1 in \cite{spall}) since we have a non-zero bias in the function evaluations, while the regular SPSA assumes the noise is zero-mean. Following this lemma, we complete the proof of Theorem \ref{thm:1spsa-conv} by invoking the well-known Kushner-Clark lemma \cite{kushner-clark}.

\todop{The bias control is with high prob and so probably all conv claims shoud be with high prob. Probably there is a simpler way out, but i dont know (yet)}
\begin{lemma}
\label{lemma:1spsa-bias}
Let $\F_n = \sigma(\theta_m,m\le n)$, $n\ge 1$.
Then, for any $i = 1,\ldots,d$, we have almost surely,  
\begin{align*}
\left| \E\left[\left.\dfrac{\overline \C_n^{\theta_n+\delta_n \Delta_n} -\overline \C_n^{\theta_n-\delta_n \Delta_n}}{2 \delta_n \Delta_n^{i}}\right| \F_n \right] - \nabla_i \C(X^{\theta_n}) \right| \xrightarrow{n \rightarrow\infty} 0.
\end{align*} 
\end{lemma}
\begin{proof}
Recall that the CPT-value estimation scheme is biased, i.e., providing samples with policy $\theta$, we obtain its CPT-value estimate as $V^{\theta}(x_0) + \epsilon^\theta$. Here $\epsilon^\theta$ denotes the bias. 
% However, since the number of samples $m_n$ go to infinity, we have $$\epsilon^{\theta_n}  \rightarrow 0 \text{ as } n \rightarrow \infty.$$

We claim
\begin{align}
&\E\left[\dfrac{\overline \C_n^{\theta_n+\delta_n \Delta_n} -\overline \C_n^{\theta_n-\delta_n \Delta_n}}{2 \delta_n \Delta_n^{i}} \left.\right| \F_n\right] \\
&= \E\left[\dfrac{\C(X^{\theta_n + \delta_n \Delta_n}) - \C(X^{\theta_n - \delta_n \Delta_n})}{2 \delta_n \Delta_n^{i}} \left.\right| \F_n\right] + \E\left[ \eta_n \mid \F_n\right],
\end{align}
where $\eta_n = \left(\dfrac{\epsilon^{\theta_n +\delta_n\Delta} - \epsilon^{\theta_n-\delta_n\Delta}}{2\delta_n\Delta_n^{i}}\right)$
 is the bias arising out of the empirical distribution based CPT-value estimation scheme.
From Proposition \ref{prop:holder-dkw} and the fact that $\frac{1}{m_n^{\alpha/2} \delta_n} \rightarrow 0$ by assumption (A3), we have that
$\eta_n$ goes to zero asymptotically. In other words,
\begin{align}
&\E\left[\dfrac{\overline \C_n^{\theta_n+\delta_n \Delta_n} -\overline \C_n^{\theta_n-\delta_n \Delta_n}}{2 \delta_n \Delta_n^{i}} \left.\right| \F_n\right] \\
&\xrightarrow{n\rightarrow\infty}  \E\left[\dfrac{\C(X^{\theta_n + \delta_n \Delta_n}) -\C(X^{\theta_n - \delta_n \Delta_n})}{2 \delta_n \Delta_n^{i}} \left.\right| \F_n\right].  \label{eq:l1}
\end{align}
We now analyse the RHS of \eqref{eq:l1}.
By using suitable Taylor's expansions,
\begin{align*}
&\C(X^{\theta_n \pm \delta_n \Delta_n}) = \C(X^{\theta_n}) \pm \delta_n \Delta_n\tr \nabla \C(X^{\theta_n}) \\
&\quad+ \frac{\delta^2}{2} \Delta_n\tr \nabla^2 \C(X^{\theta_n})\Delta_n + O(\delta_n^3).
\end{align*}
From the above, it is easy to see that 
\begin{align*}
&\dfrac{\C(X^{\theta_n + \delta_n \Delta_n}) - \C(X^{\theta_n - \delta_n \Delta_n})}{2 \delta_n \Delta_n^i}
- \nabla_i \C(X^{\theta_n})\\
&=\underbrace{\sum_{j=1,j\not=i}^{N} \frac{\Delta_n^j}{\Delta_n^i}\nabla_j \C(X^{\theta_n})}_{(I)} + O(\delta_n^2).
\end{align*}
Taking conditional expectation on both sides, we obtain
\begin{align}
&\E\left[\dfrac{\C(X^{\theta_n + \delta_n \Delta_n}) - \C(X^{\theta_n - \delta_n \Delta_n})}{2 \delta_n \Delta_n^i} \left.\right| \F_n\right] \nonumber\\
&= \nabla_i \C(X^{\theta_n}) + \E\left[\sum_{j=1,j\not=i}^{N} \frac{\Delta_n^j}{\Delta_n^i}\right]\nabla_j \C(X^{\theta_n}) + O(\delta_n^2)\nonumber\\
&=  \nabla_i \C(X^{\theta_n}) + O(\delta_n^2).\label{eq:l2}
\end{align}
The first equality above follows from the fact that $\Delta_n$ is distributed according to a $d$-dimensional vector of Rademacher random variables and is independent of $\F_n$. The second inequality follows by observing that $\Delta_n^i$ is independent of $\Delta_n^j$, for any $i,j =1,\ldots,d$, $j\ne i$. 

The claim follows by using the fact that $\delta_n \rightarrow 0$ as $n\rightarrow \infty$.
\end{proof}

\subsection*{Proof of Theorem \ref{thm:1spsa-conv}}

\begin{proof}

%Recall that $\F_n = \sigma(\theta_m,m\le n;\Delta_m,m<n)$, $n\ge 1$.
We first rewrite the update rule \eqref{eq:theta-update} as follows: For $i=1,\ldots,d$,
\begin{align}
\theta^{i}_{n+1}  =  \theta^{i}_n +  \gamma_n(\nabla_{i} \C(X^{\theta_n}) + \beta_n + \xi_n), 
\label{eq:1spsa-equiv}
\end{align}
where 
\begin{align*}
\beta_n = &\E\left(\dfrac{(\overline \C_n^{\theta_n + \delta_n \Delta_n} -\overline \C_n^{\theta_n - \delta_n \Delta_n})}{2 \delta_n \Delta_n^{i}} \mid \F_n \right) - \nabla \C(X^{\theta_n}),\\
\xi_n = & \left(\dfrac{\overline \C_n^{\theta_n + \delta_n \Delta_n} -\overline \C_n^{\theta_n - \delta_n \Delta_n}}{2 \delta_n\Delta_n^{i}}\right) \\
&\qquad - \E\left(\dfrac{(\overline \C_n^{\theta_n + \delta_n \Delta_n} -\overline \C_n^{\theta_n - \delta_n \Delta_n})}{2 \delta_n \Delta_n^{i}}\mid \F_n \right).
\end{align*}
In the above, $\beta_n$ is the bias in the gradient estimate due to SPSA and $\xi_n$ is a martingale difference sequence.
% , while  Lemma \ref{lemma:1spsa-bias} and the fact that $\delta_n \rightarrow 0$ imply that $\beta_n$ vanishes asymptotically. 

Convergence of \eqref{eq:1spsa-equiv} can be inferred from Theorem 5.3.1 on pp. 191-196 of \cite{kushner-clark}, provided we verify the necessary assumptions given as (B1)-(B5) below:
\begin{enumerate}[\bfseries (B1)]
\item $\nabla \C(X^{\theta})$ is a continuous $\R^{d}$-valued function.
\todop{Don't know how this gets verfied in our setting. Help!}
\item  The sequence $\beta_n,n\geq 0$ is a bounded random sequence with
$\beta_n \rightarrow 0$ almost surely as $n\rightarrow \infty$.
\item The step-sizes $\gamma_n,n\geq 0$ satisfy
$  \gamma_n\rightarrow 0 \mbox{ as }n\rightarrow\infty \text{ and } \sum_n \gamma_n=\infty.$
\item $\{\xi_n, n\ge 0\}$ is a sequence such that for any $\epsilon>0$,
\[ \lim_{n\rightarrow\infty} P\left( \sup_{m\geq n}  \left\|
\sum_{k=n}^{m} \gamma_k \xi_k\right\| \geq \epsilon \right) = 0. \]
\item There exists a compact subset $K$ which is the set of asymptotically stable equilibrium points for the following ODE:
\begin{align}
\dot\theta^{i}_t = \check\Gamma_{i}\left(- \nabla \C(X^{\theta^{i}_t})\right), \text{ for } i=1,\dots,d,\label{eq:pi-ode}
\end{align}
\end{enumerate} 

In the following, we verify the above assumptions for the recursion \eqref{eq:theta-update}:
\begin{itemize}
\item (B1) holds by assumption in our setting.

\item Lemma \ref{lemma:1spsa-bias} above establishes that the bias $\beta_n$ is $O(\delta_n^2)$ and since $\delta_n \rightarrow 0$ as $n\rightarrow \infty$, it is easy to see that (B2) is satisfied for $\beta_n$. 

\item (B3) holds by assumption (A3).

\item We verify (B4) using arguments similar to those used in \cite{spall} for the classic SPSA algorithm:\\
We first recall Doob's martingale inequality (see (2.1.7) on pp. 27 of \cite{kushner-clark}):
\begin{align}
\Prob{ \sup_{m\geq 0}   \left\|W_l\right\| \geq \epsilon } \le \dfrac{1}{\epsilon^2} \lim_{l\rightarrow \infty} \E \left\|W_l\right\|^2. 
\end{align}
Applying the above inequality to the martingale sequence $\{W_l\}$, where  $W_l := \sum_{n=0}^{l-1} \gamma_n \eta_n$, $l\ge 1$, we obtain
\begin{align}
&\Prob{ \sup_{l\geq k}   \left\|\sum_{n=k}^{l} \gamma_n \xi_n\right\| \geq \epsilon } \le \dfrac{1}{\epsilon^2} \E \left\|
\sum_{n=k}^{\infty} \gamma_n \xi_n\right\|^2 \\
&= \dfrac{1}{\epsilon^2} \sum_{n=k}^{\infty} \gamma_n^2 \E\left\| \eta_n\right\|^2. \label{eq:b4}
\end{align}
The last equality above follows by observing that, for $m < n$, $\E(\xi_m \xi_n) = \E(\xi_m \E(\xi_n\mid \F_n))=0$.
We now bound $\E\left\| \xi_n\right\|^2$ as follows:
\begin{align}
&\E\left\| \xi_n\right\|^2\le \E\left(\dfrac{\overline \C_n^{\theta_n + \delta_n \Delta_n} -\overline \C_n^{\theta_n - \delta_n \Delta_n}}{2 \delta_n \Delta_n^i}\right)^2 \label{eq:mi}\\
\le &\left(\left(\E\left(\dfrac{\overline \C_n^{\theta_n + \delta_n \Delta_n}}{2 \delta_n \Delta_n^i}\right)^2\right)^{1/2}\right.\\ 
&\qquad\left.+ \left(\E\left(\dfrac{\overline \C_n^{\theta_n - \delta_n \Delta_n}}{2 \delta_n \Delta_n^i}\right)^2\right)^{1/2}\right)^2 \label{eq:minko}\\
\le &\frac1{4\delta_n^2} \left[ \E\left(\frac1{(\Delta_n^i)^{2+2\alpha_1}}\right) \right]^{\frac{1}{1+\alpha1}} \nonumber\\
& \times \left(\left[\E\left[ (\overline \C_n^{\theta_n + \delta_n \Delta_n})\right]^{2+2\alpha_2}\right]^{\frac{1}{1+\alpha_2}} \right.\\
&\left.\qquad+
\left[\E\left[ (\overline \C_n^{\theta_n - \delta_n \Delta_n})\right]^{2+2\alpha_2}\right]^{\frac{1}{1+\alpha_2}}\right)\label{eq:holder}\\
\le &\frac1{4\delta_n^2} \left(\left[\E\left[ (\overline \C_n^{\theta_n + \delta_n \Delta_n})\right]^{2+2\alpha_2}\right]^{\frac{1}{1+\alpha_2}} \right.\\
&\left.\qquad  +
\left[\E\left[ (\overline \C_n^{\theta_n - \delta_n \Delta_n})\right]^{2+2\alpha_2}\right]^{\frac{1}{1+\alpha_2}}\right) \label{eq:h2}\\
\le & \frac{C}{\delta_n^2}, \text{ for some } C< \infty. \label{eq:h3}
\end{align}
The inequality in \eqref{eq:mi} uses the fact that, for any random variable $X$, $\E\left\|X -  E[X\mid\F_n]\right\|^2 \le \E X^2$. The inequality in \eqref{eq:minko} follows by the fact that $\E (X+Y)^2 \le \left( (\E X^2)^{1/2} + (\E Y^2)^{1/2}\right)^2$.
The inequality in \eqref{eq:holder} uses Holder's inequality, with $\alpha_1, \alpha_2>0$ satisfying $\frac{1}{1+\alpha_1} + \frac{1}{1+\alpha_2}=1$. 
The equality in \eqref{eq:h2} above follows owing to the fact that $\E\left(\frac1{(\Delta_n^i)^{2+2\alpha_1}}\right)  = 1$ as $\Delta_n^i$ is Rademacher. 
The inequality in \eqref{eq:h3} follows by using the fact that
%, for any $\theta$, the CPT-value estimate $\widehat \C(D^\theta) = \C(D^\theta) + \epsilon^\theta$. We assume a finite state-action spaced SSP (which implies that the costs $\max_{s,a} g(s,a) < \infty$) and consider only \textit{proper} policies (which implies that the total cost $D^\theta$ is bounded for any policy $\theta$) and finally, by (A1), the weight functions are Lipschitz - these together imply 
$\C(D^\theta)$ is bounded for any policy $\theta$ and the bias $\epsilon^\theta$ is bounded by Proposition \ref{prop:holder-dkw}.
\todop{Need to update the above arguments for the general case of $X^\theta$, with $\theta$ in a compact set}

Thus, $\E\left\| \xi_n\right\|^2 \le \frac{C}{\delta_n^2}$ for some $C<\infty$. Plugging this in \eqref{eq:b4}, we obtain
\begin{align*}
 \lim_{k\rightarrow\infty} P\left( \sup_{l\geq k}   \left\|\sum_{n=k}^{l} \gamma_n \xi_n\right\|\geq \epsilon \right) \le \dfrac{d C}{\epsilon^2} \lim_{k\rightarrow\infty} \sum_{n=k}^{\infty}  \frac{\gamma_n^2}{\delta_n^2} =0.
\end{align*}
The equality above follows from (A3) in the main paper.
\item Observe that $\C(X^{\theta})$ serves as a strict Lyapunov function for the ODE \eqref{eq:pi-ode}. This can be seen as follows:
$$ \dfrac{d \C(X^{\theta})}{dt} = \nabla \C(X^{\theta}) \dot \theta = \nabla \C(X^{\theta}) \check\Gamma \left(-\nabla \C(X^{\theta}\right) < 0.$$
Hence, the set $\K = \{\theta \mid \check\Gamma_{i} \left(-\nabla \C(X^{\theta})\right)=0, \forall i=1,\ldots,d\}$ serves as the asymptotically stable attractor for the ODE \eqref{eq:pi-ode}.
\end{itemize} 
The claim follows from the Kushner-Clark lemma.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proofs for CPT-SPSA-N}
\label{sec:proofs-spsa-n}
To simplify notation, we will use $X^{+}$ (resp. $X^{-}$) to denote $X^{\theta_n+\delta_n(\Delta_n+\widehat\Delta_n)}$ (resp. $X^{\theta_n-\delta_n(\Delta_n+\widehat\Delta_n)}$) in the proofs below.

Before proving Theorem \ref{thm:2spsa}, we bound the bias in the SPSA based estimate of the Hessian in the following lemma.
\begin{lemma}
\label{lemma:2spsa-bias}
For any $i, j= 1,\ldots,d$, we have almost surely,  
\begin{align*}
&    \left| \E\left[\left.\dfrac{\overline \C_n^{\theta_n+\delta_n(\Delta_n+\widehat\Delta_n)} + \overline \C_n^{\theta_n-\delta_n(\Delta_n+\widehat\Delta_n)} - 2 \overline \C_n^{\theta_n}}{\delta_n^2 \Delta_n^{i}\widehat\Delta_n^j}\right| \F_n \right] \right.\\
		&\left.\qquad- \nabla^2_{i,j} \C(X^{\theta_n}) \right| \xrightarrow{n \rightarrow\infty} 0.
\end{align*} 
\end{lemma}
\begin{proof}
As in the proof of Lemma \ref{lemma:1spsa-bias}, we can ignore the bias from the CPT-value estimation scheme and conclude that
\begin{align}
    &\E\left[\dfrac{\overline \C_n^{\theta_n+\delta_n(\Delta_n+\widehat\Delta_n)} + \overline \C_n^{\theta_n-\delta_n(\Delta_n+\widehat\Delta_n)} - 2\overline \C_n^{\theta_n}}{\delta_n^2 \Delta_n^i\widehat\Delta_n^j} \left.\right| \F_n\right]  \nonumber\\
     & \xrightarrow{n\rightarrow\infty} \E\left[\dfrac{\C(X^{+}) + \C(X^{-}) - 2\C(X^{\theta_n})}{\delta_n^2 \Delta_n^i\widehat\Delta_n^j} \left.\right| \F_n\right].  \label{eq:l21}
\end{align}
Now, the RHS of \eqref{eq:l21} approximates the true gradient with only an $O(\delta_n^2)$ error; this can be inferred using arguments similar to those used in the proof of Proposition 4.2 of \cite{bhatnagar2015simultaneous}. We provide the proof here for the sake of completeness.
Using Taylor's expansion as in Lemma \ref{lemma:1spsa-bias}, we obtain
\begin{align*}
&\dfrac{\C(X^{+}) + \C(X^{-}) - 2\C(X^{\theta_n})}{\delta_n^2 \Delta_n^i\widehat\Delta_n^j}\\
&=  \frac{(\Delta_n+\hat{\Delta_n})\tr \nabla^2 \C(X^{\theta_n})(\Delta_n
+\hat{\Delta_n})}{\triangle_i(n)\hat{\triangle}_j(n)}
+ O(\delta_n^2) \\
&= \sum_{l=1}^{d}\sum_{m=1}^{d} \frac{\Delta_n^l\nabla^2_{l,m}\C(X^{\theta_n})\Delta_n^m}{\Delta_n^i
\hat{\Delta}_n^j}\\
& + 2\sum_{l=1}^{d}\sum_{m=1}^{d} \frac{\Delta_n^l\nabla^2_{l,m}
\C(X^{\theta_n})\hat{\Delta}_n^m}{\Delta_n^i
\hat{\Delta}_n^j}\\
&+ \sum_{l=1}^{d}\sum_{m=1}^{d} \frac{\hat{\Delta}_n^l\nabla^2_{l,m}\C(X^{\theta_n})\hat{\Delta}_n^m}{\Delta_n^i
\hat{\Delta}_n^j} + O(\delta_n^2).
\end{align*}
Taking conditional expectation, we observe that the first and last term above become zero, while the second term becomes $\nabla^2_{ij}
\C(X^{\theta_n})$. The claim follows by using the fact that $\delta_n \rightarrow 0$ as $n\rightarrow \infty$.
\end{proof}

\begin{lemma}
\label{lemma:2spsa-grad}
For any $i = 1,\ldots,d$, we have almost surely,  
\begin{align*}
\left| \E\left[\left.\dfrac{\overline \C_n^{\theta_n + \delta_n (\Delta_n + \hat \Delta_n)} -\overline \C_n^{\theta_n - \delta_n (\Delta_n + \hat \Delta_n)}}{2 \delta_n \Delta_n^i}\right| \F_n \right] - \nabla_i \C(X^{\theta_n}) \right|\\
 \rightarrow 0 \text{ as } n\rightarrow\infty.
\end{align*} 
\end{lemma}
\begin{proof}
%As in the proof of Lemma \ref{lemma:1spsa-bias}, we can ignore the bias from the CPT-value estimation scheme and conclude that
%\begin{align*}
%\quad&\E\left[\dfrac{\overline \C_n^{\theta_n + \delta_n (\Delta_n+\widehat\Delta_n)} -\overline \C_n^{\theta_n - \delta_n (\Delta_n+\widehat\Delta_n)}}{2 \delta_n \Delta_n^i} \left.\right| \F_n\right] 
%\xrightarrow{n\rightarrow\infty}  \E\left[\dfrac{\C(X^{\theta_n + \delta_n \Delta_n}) -\C(X^{\theta_n - \delta_n \Delta_n})}{2 \delta_n \Delta_n^i} \left.\right| \F_n\right].  
%\end{align*}
%The rest of the proof amounts to showing that the RHS of the above approximates the true gradient with an $O(\delta_n^2)$ correcting term; this can be done in a similar manner as the proof of Lemma \ref{lemma:1spsa-bias}.
Follows by using completely parallel arguments to that in Lemma \ref{lemma:1spsa-bias}.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The following lemma establishes that the Hessian recursion \eqref{eq:2spsa-H} converges to the true Hessian, for any policy $\theta$.

\begin{lemma}
\label{lemma:h-est}
For any $i, j= 1,\ldots,d$, we have almost surely,  
\begin{align*}
&\left \| H^{i, j}_n - \nabla^2_{i,j} \C(X^{\theta_n}) \right \| \rightarrow 0,
\text{ and }\\
&\left \| \Upsilon(\overline H_n)^{-1} - \Upsilon(\nabla^2_{i,j} \C(X^{\theta_n}))^{-1} \right \| \rightarrow 0.
\end{align*}
\end{lemma}
\begin{proof}
 Follows in a similar manner as in the proofs of Lemmas 7.10 and 7.11 of \cite{Bhatnagar13SR}.
\end{proof}

\begin{proof}\textbf{\textit{(Theorem \ref{thm:2spsa})}}
The proof follows in a similar manner as the proof of Theorem 7.1 in \cite{Bhatnagar13SR}; we provide a sketch below for the sake of completeness.

We first rewrite the recursion \eqref{eq:2spsa} as follows:
For $i=1,\ldots, d$
\begin{align}
 &\theta^{i}_{n+1} = \Gamma_{i}\left(\theta^{i}_n + \gamma_n \sum_{j=1}^{d} \bar M^{i,j}(\theta_n) \nabla_{j} \C(X^\theta_n) + \gamma_n \zeta_n \right.\\
&\qquad\qquad\qquad\left. + \chi_{n+1} - \chi_n \right), \label{eq:pi-n}
\end{align}
where 
\begin{align*}
&\bar M^{i,j}(\theta) = \Upsilon(\nabla^2 \C(X^{\theta}))^{-1},\\
& \chi_n = \sum_{m=0}^{n-1} \gamma_m \sum_{k=1}^{d} {\bar{M}}_{i,k}(\theta_m)\Bigg(
\frac{\C(X^{-}) -
\C(X^{+})}{2\delta_m \Delta^k_m} 
 \\
 &- E\left[\frac{\C(X^{-}) -
\C(X^{+})}{2\delta_m \Delta^k_m} 
\mid {\cal F}_m\right]\Bigg) \text{ and}\\
&\zeta_n \!=\! \E\!\left[\left.\dfrac{\overline \C_n^{\theta_n + \delta_n (\Delta_n + \hat \Delta_n)} \!-\!\overline \C_n^{\theta_n - \delta_n (\Delta_n + \hat \Delta_n)}}{2 \delta_n \Delta_n^i}\right| \F_n \right] \!-\! \nabla_i \C(X^{\theta_n}).
\end{align*}
In lieu of Lemmas \ref{lemma:2spsa-bias}--\ref{lemma:h-est}, it is easy to conclude that $\zeta_n \rightarrow 0$ as $n\rightarrow \infty$, $\chi_n$ is a martingale difference sequence and that $\chi_{n+1} - \chi_n \rightarrow 0$ as $n\rightarrow \infty$. 
Thus, it is easy to see that \eqref{eq:pi-n} is a discretization of the ODE:
\begin{align}
\dot\theta^{i}_t = \check\Gamma_{i}\left(- \nabla \C(X^{\theta^{i}_t}) \Upsilon(\nabla^2 \C(X^{\theta_t}))^{-1} \nabla \C(X^{\theta^{i}_t}) \right).
\label{eq:n-ode}
\end{align}
Since $\C(X^{\theta})$ serves as a Lyapunov function for the ODE \eqref{eq:n-ode}, it is easy to see that the set \\$\K = \{\theta \mid
\nabla \C(X^{\theta^{i}})  \check\Gamma_{i}\left(-\Upsilon(\nabla^2 \C(X^{\theta}))^{-1} \nabla \C(X^{\theta^{i}})\right)
=0, \forall i=1,\ldots,d\}$ is an asymptotically stable attractor set for the ODE \eqref{eq:n-ode}. The claim now follows from Kushner-Clark lemma.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
