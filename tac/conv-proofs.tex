%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proofs for CPT-value estimator}
\label{appendix:cpt-est}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\holder continuous weights}
\label{sec:holder-proofs}
For proving Propositions \ref{prop:holder-asymptotic} and \ref{prop:sample-complexity-discrete}, we require Hoeffding's inequality, which is given below.
\begin{lemma}
Let $Y_1,...Y_n$ be independent random variables satisfying $\Prob{a\leq Y_i \leq b}= 1,$ for each $i$, where $a<b.
$ Then for $t>0$,
$$\Prob{\left|\sum_{i=1}^n Y_i -\sum_{i=1}^n E(Y_i)\right| \geq nt } \leq 2\exp{\{-2nt^2 /(b-a)^2\}}. $$
\end{lemma}

\begin{proposition}
\label{prop:Holder-cpt-finite}
Under (A1), the CPT-value $\C(X)$ as defined by \eqref{eq:cpt-general} is finite. 
\end{proposition}
\begin{proof}

H\"{o}lder continuity of $w^+$ and the fact that $w^+(0)=0$ imply that 
\begin{align*}
&\int_0^{\infty} w^+\left(\Prob{u^{+}(X)>z}\right) dz 
\le H \int_0^{\infty} \mathbb{P}^{\alpha} \left(u^+(X)>z\right) dz\\
&\le H \int_0^{\infty} \mathbb{P}^{\gamma} \left(u^+(X)>z\right) dz 
<\infty.
\end{align*}
The second inequality is valid since $\Prob{u^+(X)>z} \leq 1$. The claim follows for the first integral in \eqref{eq:cpt-general}, and the finiteness of the second integral in \eqref{eq:cpt-general} can be argued in an analogous fashion.
\end{proof}

%%%%
\begin{proposition}
\label{prop:holder-quantile}
Assume (A1). Let $\xi^+_{\frac{i}{n}}$ and $\xi^-_{\frac{i}{n}}$ denote the $\frac{i}{n}$th quantile of $u^+(X)$ and $u^-(X)$, respectively. Then, we have  
%\todoc{Fix the ugly parenthesis! Also, why do we suddenly number all equations as if they all needed a number??}
\begin{align}
&\lim_{n \rightarrow \infty} \sum_{i=1}^{n-1} \xi^+_{\frac{i}{n}} \left(w^+\left(\frac{n-i}{n}\right)- w^+\left(\frac{n-i-1}{n}\right) \right)  \nonumber\\
&\qquad= \int_0^{\infty} w^+\left(\Prob{u^+(X)>z}\right) dz < \infty,\label{eq:simple-estimation}\\
&\lim_{n \rightarrow \infty} \sum_{i=1}^{n-1} \xi^-_{\frac{i}{n}} \left(w^-\left(\frac{i}{n}\right)- w^-\left(\frac{i-1}{n}\right) \right) \nonumber\\
&\qquad= \int_0^{\infty} w^-\left(\Prob{u^-(X)>z}\right) dz < \infty.\label{eq:simple-estimation2}
\end{align}
\end{proposition}

%\todoj[inline]{Fix the upper limit..I think it should run up to $n$}

\begin{proof}
We shall focus on proving the first part of equation \eqref{eq:simple-estimation}. Notice that the following linear combination of simple functions: 
\begin{align}
\sum_{i=1}^{n-1} w^+ \left(\frac{i}{n}\right) 
I_{\left[\xi^+_\frac{n-i-1}{n}, \xi^+_\frac{n-i}{n}\right]}(z)
\label{eq:simplew}
\end{align}
will converge almost everywhere to the function $w^+\left(\Prob{u^{+}(X)>z}\right)$ in the interval $[0, \infty)$. Further, for all $z \in [0,\infty)$, we have
\begin{align}
\sum_{i=1}^{n-1} w^+ \left(\frac{i}{n}\right) 
I_{\left[\xi^+_\frac{n-i-1}{n}, \xi^+_\frac{n-i}{n}\right]}(z)
<
w^+\left(\Prob{u^{+}(X)>z}\right).
\end{align}

The integral of \eqref{eq:simplew} can be simplified as follows:
\begin{align*}
& \int_0^{\infty} \sum_{i=0}^{n} w^+ \left(\frac{i}{n}\right) 
 I_{\left[\xi^+_\frac{n-i-1}{n}, \xi^+_\frac{n-i}{n}\right]}(z) dz \\
 & = \sum_{i=0}^{n-1} w^+\left(\frac{i}{n}\right) \left(\xi^+_{\frac{n-i}{n}} -
\xi^+_{\frac{n-i-1}{n}}\right) \\ & = \sum_{i=0}^{n-1} \xi^+_{\frac{i}{n}} \left(w^+\left(\frac{n-i}{n}\right)-
    w^+\left(\frac{n-i-1}{n}\right)\right).
\end{align*}
The H\"{o}lder continuity property assures the fact that 
$\lim_{n \rightarrow \infty}  | w^+(\frac{n-i}{n})- w^+(\frac{n-i-1}{n})| =0$, and the limit in \eqref{eq:simple-estimation} holds through an application of the dominated convergence theorem.

The second part of \eqref{eq:simple-estimation} can be justified in a similar fashion.
\end{proof} 
%%%

%%%%%%%%%\xi^+
\subsection*{Proof of Proposition \ref{prop:holder-asymptotic}}


\begin{proof}
Without loss of generality, assume the  \holder constants $H$ of the weight functions $w^+$ and $w^-$ are both  
$\alpha$.
We prove the claim for the first integral in the CPT-value estimator $\overline \C_n$ in Algorithm \ref{alg:holder-est}, i.e., we show  that 
%\todoc{Don't start sentences with ''and'' or ''or''.}
\begin{align}
&\lim_{n\rightarrow \infty} \sum_{i=1}^{n} u^+\left(X_{[i]}\right) \left(w^+\left(\frac{n-i+1}{n}\right)- w^+\left(\frac{n-i}{n}\right)\right)\nonumber\\
&\xrightarrow{n \rightarrow\infty} \int_0^{\infty} w^+\left(P\left(u^+(X)>z\right)\right) dz , \text{ a.s.} 
\label{eq:claim11}
\end{align}


The main part of the proof is focused on finding an upper bound of the probability
\begin{align*}
\mathbb{P} \left( \left| \sum_{i=1}^{n-1} u^+\left(X_{[i]}\right)  \left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right) \right.\right.\\
\quad\left.\left. -
\sum_{i=1}^{n-1} \xi^+_{\frac{i}{n}}  \left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right) \right| >
\epsilon\right),
\end{align*}
Observe the fact that 
\begin{align*}
&\sum_{i=1}^{n} u^+\left(X_{[i]}\right) \left(w^+\left(\frac{n-i+1}{n}\right)- w^+\left(\frac{n-i}{n}\right)\right)\nonumber\\
&-\sum_{i=1}^{n-1} u^+\left(X_{[i]}\right) \left(w^+\left(\frac{n-i}{n}\right)- w^+\left(\frac{n-i-1}{n}\right)\right)\\
&=\sum_{i=1}^{n} \left(u^+\left(X_{[i]}\right) - u^+\left(X_{[i-1]}\right)\right) w^+\left(\frac{n+1-i}{n}\right)\\
&-\sum_{i=1}^{n} \left(u^+\left(X_{[i]}\right) - u^+\left(X_{[i-1]}\right)\right)w^+\left(\frac{n-i}{n}\right)\\
&=\sum_{i=1}^{n} \left(u^+\left(X_{[i]}\right) - u^+\left(X_{[i-1]}\right)\right)\\
&\qquad\times \left(w^+\left(\frac{n+1-i}{n}\right) 
-w^+\left(\frac{n-i}{n}\right)  \right) \\
&\leq u^+\left(X_{[n]}\right) \times \frac{1}{n^{\alpha}}
\end{align*}

Under (A1), the term $u^+\left(X_{[n]}\right) w^+(\frac{1}{n})$ converges to 0,
hence in conjunction with proposition \ref{prop:holder-quantile}, it suffices to show that
\begin{align*}
& \mathbb{P} \left( \left| \sum_{i=1}^{n-1} u^+\left(X_{[i]}\right) \left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right) \right.\right. \\
&\left.\left.-\sum_{i=1}^{n-1} \xi^+_{\frac{i}{n}} \left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right)\right| >
\epsilon \right)\\
&=0, 
\end{align*}
For any given $\epsilon>0$, we have 
\begin{align*}
& \mathbb{P} \left( \left| \sum_{i=1}^{n-1} u^+\left(X_{[i]}\right)  \left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right) \right.\right. \\
&\qquad\left.\left.-\sum_{i=1}^{n-1} \xi^+_{\frac{i}{n}}  \left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right) \right| >
\epsilon\right) \\ 
%%%%%%%%%%%%%%%%%%%%%%%%
& \leq \mathbb{P}\left( \bigcup _{i=1}^{n-1} \left\{ \left| u^+\left(X_{[i]}\right) \left(w^+\left(\frac{n-i}{n}\right) -
w^+\left(\frac{n-i-1}{n}\right)\right)\right.\right.\right.\\
&\qquad\left.\left.\left.- \xi^+_{\frac{i}{n}}  \left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right)
\right| > \frac{\epsilon}{n-1} \right\}\right) \\ 
% %%%%%%%%%%%
& \leq \sum _{i=1}^{n-1} \mathbb{P}\left( \left| u^+\left(X_{[i]}\right) 
\left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right)\right.\right. \\
&\qquad\left.\left.- \xi^+_{\frac{i}{n}}  \left(w^+\left(\frac{n+1-i}{n}\right) -
w^+\left(\frac{n-i}{n}\right)\right) \right| > \frac{\epsilon}{n-1}\right) \\ 
% %%%%%%%%%%%
& = \sum _{i=1}^{n-1} \mathbb{P}\left( \left| \left( u^+\left(X_{[i]}\right) -
\xi^+_{\frac{i}{n}}\right)\right.\right. \\
&\quad\qquad\left.\left.\times\left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right) \right| > \frac{\epsilon}{n-1}\right)
\\ 
& \leq \sum _{i=1}^{n-1} \Prob{ \left| \left( u^+\left(X_{[i]}\right) - \xi^+_{\frac{i}{n}}\right) \left(\frac{1}{n}\right)^{\alpha}
\right| > \frac{\epsilon}{n-1}} \stepcounter{equation}\tag{\theequation}\label{eq:bd02}\\ 
& \le \sum _{i=1}^{n-1} \Prob{ \left| \left( u^+\left(X_{[i]}\right) - \xi^+_{\frac{i}{n}}\right)
\right| > \frac{\epsilon}{n^{1-\alpha}}}.\stepcounter{equation}\tag{\theequation}\label{eq:bd12}
\end{align*}
In the above, \eqref{eq:bd02} follows from the fact that $w^+$ is \holder with constant $1$.

Now we find the upper bound of the probability of a single term in the sum above, i.e.,
\begin{align*}
& \Prob{ \left | u^+\left(X_{[i]}\right)\! -\! \xi^+_{\frac{i}{n}} \right | \!>\! \frac {\epsilon} {n^{\left(1-\alpha\right)}}}  
 \!=\! \Prob{
    u^+\left(X_{[i]}\right) \!-\! \xi^+_{\frac{i}{n}} \!>\! \frac {\epsilon} {n^{\left(1-\alpha\right)}}}\\
	&	+ \Prob{ u^+\left(X_{[i]}\right) -
    \xi^+_{\frac{i}{n}} < - \frac {\epsilon} {n^{\left(1-\alpha\right)}}}.
\end{align*} 

We focus on the first term above.
$$\hspace{-6em}\text{Let } W_j = I_{\left(u^+(X_j) > \xi^+_{\frac{i}{n}} + \frac{\epsilon}{n^{(1-\alpha)}}\right)}, j=1, \ldots,n.$$ Using the fact that a probability distribution function is non-decreasing, we obtain 
\begin{align*}
& \Prob { u^+(X_{[i]}) - \xi^+_{\frac{i}{n}} > \frac {\epsilon}{\nalpha}}  \\
& = \Prob { \sum _{j=1}^{n} W_j > n\left(1-\frac{i}{n}\right)} \\ 
& = \mathbb{P} \left( \sum _{j=1}^{n} W_j - n  \left[1-F^{+}\left(\xi^+_{\frac{i}{n}}
+\frac{\epsilon}{n^{(1-\alpha)}}\right)\right] \right. \\
&\left.\quad\qquad> n  \left[F^{+}\left(\xi^+_{\frac{i}{n}} +\frac{\epsilon}{n^{(1-\alpha)}}\right)
- \frac{i}{n}\right]\right).
\end{align*}

Using the fact that 
$E W_j = 1-F^{+}\left(\xi^+_{\frac{i}{n}} +\frac{\epsilon}{n^{(1-\alpha)}}\right)$ in conjunction with Hoeffding's inequality, we obtain
\begin{align*}
&\mathbb{P} \left( \sum _{i=1}^{n} W_j - n  \left[1-F^{+}\left(\xi^+_{\frac{i}{n}} +\frac{\epsilon}{n^{(1-\alpha) }  } \right) \right] \right.\\
&\left.\qquad> n
 \left[F^{+}\left(\xi^+_{\frac{i}{n}} +\frac{\epsilon}{n^{(1-\alpha)} } \right) - \frac{i}{n}\right]\right) \le e^{-2n
\delta^{'}_i},
\end{align*}
where $\delta^{'}_i = F^{+}\left(\xi^+_{\frac{i}{n}} +\frac{\epsilon} {n^{(1-\alpha)} }\right) - \frac{i}{n}$. Since 
$F^{+}$ is Lipschitz, we have that $ \delta^{'}_i \leq L^{+} \left(\frac{\epsilon}{\nalpha}\right)$.
Hence, we obtain
\begin{align}
\Prob{ u^+(X_{[i]}) - \xi^+_{\frac{i}{n}} > \frac {\epsilon}{\nalpha}} &\le e^{-2n L^{+}
\frac{\epsilon}{\nalpha} } \nonumber\\
&= e^{-2n^\alpha  L ^{+} \epsilon}
\label{eq:a12}
\end{align}
In a  similar fashion, one can show that 
\begin{align}
\Prob{ u^+(X_{[i]}) -\xi^+_{\frac{i}{n}} < -\frac {\epsilon} {\nalpha}} \leq e^{-2n^\alpha  L^{+}  \epsilon}.
\label{eq:a23}
\end{align}
Combining \eqref{eq:a12} and \eqref{eq:a23},  we obtain 
%\todoc{Is not $\mathbb{N} \cap (0,1) =\emptyset$???}
\begin{align*}
\Prob{ \left| u^+(X_{[i]}) -\xi^+_{\frac{i}{n}} \right| < -\frac {\epsilon} {\nalpha}} \leq 2
e^{-2n^\alpha  L^{+} \epsilon} , 
\end{align*}
Plugging the above in \eqref{eq:bd12}, we obtain
\begin{align}
&
\mathbb{P} \left( \left| \sum_{i=1}^{n-1} u^+\left(X_{[i]}\right)  \left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right) \right.\right. \nonumber\\
&\left.\left.\quad -
\sum_{i=1}^{n-1} \xi^+_{\frac{i}{n}} \left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right) \right| >
\epsilon\right) \nonumber\\
&\leq 2(n-1) e^{-2n^\alpha  L^{+}\epsilon} \leq 2ne^{-2n^\alpha  L^{+}\epsilon}.\label{eq:holder-sample-complexity-extract}
\end{align}

Notice that $\sum_{n=1}^{\infty}  2n  e^{-2n^{\alpha} L^{+} \epsilon}< \infty$ since the sequence 
$2n e^{-2n^{\alpha} L^{+}}$ will decrease more rapidly than the sequence
$\frac{1}{n^k}$, $\forall k>1$.

By applying the Borel Cantelli lemma,  $\forall \epsilon >0$, we have that
\begin{align*}
&\mathbb{P} \left( \left| \sum_{i=1}^{n-1} u^+\left(X_{[i]}\right)  \left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right)\right.\right. \\
&\left.\left.-
\sum_{i=1}^{n-1} \xi^+_{\frac{i}{n}}  \left(w^+\left(\frac{n-i}{n} \right)  - w^+\left(\frac{n-i-1}{n} \right) \right) \right| >
\epsilon , i.o.\right) \\
&=0, 
\end{align*}
which implies  \eqref{eq:claim11}. 

The proof of 
$\C_{n}^{-} \rightarrow \C^{-}(X)$ follows in a similar manner as above by replacing $u^+(X_{[i]})$ by $u^-(X_{[n-i]})$, after observing that $u^{-}$ is decreasing, which in turn implies that
$u^-(X_{[n-i]})$ is an estimate of the quantile $\xi^{-}_{\frac{i}{n}}$.
\end{proof}


\subsection*{Proof of Proposition \ref{prop:holder-dkw}}
For proving Proposition \ref{prop:holder-dkw}, we require the following well-known inequality that provides a finite-time bound on the distance between the empirical distribution and the true distribution:
\begin{lemma}{\textbf{\textit{(Dvoretzky-Kiefer-Wolfowitz (DKW) inequality)}}}\\
Let ${\hat F_n}(u)=\frac{1}{n} \sum_{i=1}^n I_{\left[(u(X_i)) \leq u\right]}$ denote the empirical distribution of a r.v. $u(X)$, with $u(X_1),\ldots,u(X_n)$ being sampled from the r.v $u(X)$.
The, for any $n$ and $\epsilon>0$, we have
$$
\Prob{\sup_{x\in \mathbb{R}}|\hat{F}_n(x)-F(x)|>\epsilon } \leq 2 e^{-2n\epsilon^2}.
$$
\end{lemma}

The reader is referred to Chapter 2 of \cite{wasserman2006} for a detailed description of empirical distributions in general and the DKW inequality in particular.

\begin{proof}
We prove the $w^+$ part, and the $w^-$ part follows in a similar fashion.
Since $u^+(X)$ is bounded above by $M$ and $w^+$ is H\"{o}lder-continuous, we have
\begin{align*}
&\left|\int_0^{\infty} w^+\left(\Prob{u^+(X)>t}\right) dt- \int_0^{\infty} w^+\left(1- {\hat F^+_n}(t)\right) dt\right| \\ = &
    \left|\int_0^M w^+\left(\Prob{u^+(X)>t}\right) dt- \int_0^M w^+\left(1- {\hat F^+_n}(t)\right) dt\right| \\
\leq& \left|\int_0^M H |\Prob{u^+(X)<t}-{\hat F^+_n}(t)|^\alpha dt\right|\\ \leq& HM\sup_{x\in
\mathbb{R}}\left|\Prob{u^+(X)<t}-{\hat F^+_n}(t)\right|^\alpha.
\end{align*}
Now, plugging in the DKW inequality, we obtain
\begin{align}
&\mathbb{P}\left(\left|\intinfinity w^+\left(\Prob{u^+(X)>t}\right) dt \right.\right.\nonumber\\
&\quad\left.\left.- \intinfinity w^+\left(1- {\hat F^+_n}(t)\right) dt\right|>\epsilon\right)
\nonumber\\
& \leq \Prob{HM\sup_{t\in \mathbb{R}} \left|(\Prob{u^+(X)<t}-{\hat F^+_n}(t)\right|^\alpha>\epsilon}\nonumber\\
&
\leq  2 e^{-2n \left(\frac{\epsilon}{HM} \right)^\frac{2}{\alpha}}.\label{eq:dkw3}
\end{align}
The claim follows.
\end{proof}

\subsection*{Proof of Corollary \ref{cor:holder-dkw}}
\begin{proof}
 Integrating the high-probability bound in Proposition \ref{prop:holder-dkw}, we obtain
 \begin{align*}
& \E \left|\overline \C_n- \C(X) \right|  
  \le \intinfinity \Prob{\left|\overline \C_n- \C(X) \right| \geq  \epsilon} d\epsilon\\
  &\le 4 \intinfinity \exp\left(-2n\left(\epsilon/HM\right)^{2/\alpha}\right) d\epsilon \le \frac{8HM \Gamma\left(\alpha/2\right)}{n^{\alpha/2}}.
 \end{align*}
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%Lipschitz-starts-here %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Lipschitz continuous weights}
\label{sec:lipschitz-proofs}
%  \todoj{Can't we have a unified proof of \holder and Lipschitz cases?}
 Setting $\alpha=\gamma=1$ in the proof of Proposition \ref{prop:lipschitz}, it is easy to see that the CPT-value \eqref{eq:cpt-general} is finite. 

Next, in order to prove the asymptotic convergence claim in Proposition \ref{prop:lipschitz}, we require the dominated convergence theorem in its generalized form, which is provided below.
\begin{theorem}{\textbf{\textit{(Generalized Dominated Convergence theorem)}}}
Let $\{f_n\}_{n=1}^\infty$ be a sequence of measurable functions on a measurable space $E$ that converge pointwise a.e. on $E$ to $f$.  Suppose there is a sequence $\{g_n\}$ of integrable functions on $E$ that converge pointwise a.e. on $E$ to $g$ such that $|f_n| \leq g_n$ for all $n \in \mathbb{N}$.  
If $\lim\limits_{n \rightarrow \infty}$ $\int_E$ $g_n$ = $\int_E$ $g$, then $\lim\limits_{n \rightarrow \infty}$ $\int_E$ $f_n$ = $\int_E$ $f$.
\end{theorem}
\begin{proof}
This is a standard result that can be found in any textbook on measure theory. For instance, see Theorem 2.3.11 in \cite{athreya2006measure}.
\end{proof}


\subsection*{Proof of Proposition \ref{prop:lipschitz}}
\begin{proof}
We first prove the asymptotic convergence claim for the first integral  \eqref{eq:cpt-est-appendix} in the CPT-value estimator in Algorithm \ref{alg:holder-est}, i.e., we show
\begin{align}
\intinfinity w^+\left(1-{\hat F_n}^+\left(x\right)\right)  dx \rightarrow \intinfinity w^+\left(\Prob{u^+\left(X\right)>x}\right) dx.\label{eq:3}
\end{align} 

Since $w^+$ is Lipschitz continuous with, say, constant $L$, we have almost surely that
$w^{+}\left(1-\hat{F}_n\left(x\right)\right) \leq L \left(1-\hat{F}_n\left(x\right)\right)$,  
for all $n$ and 
 $w^{+}\left(\Prob{u^+\left(X\right)>x}\right) \leq L \left(\Prob{u^+\left(X\right)>x}\right)$, since $w^+\left(0\right)=0$.
 
% Notice that the empirical distribution function 
% ${\hat F_n}^+\left(x\right)$ 
% generates a Stieltjes measure which takes mass 
% $1/n$ on each of the sample points $u^+\left(X_{i}\right)$. 

We have
\begin{align}
\intinfinity \left(\Prob{u^+\left(X\right)>x}\right)  dx = \EE{u^+\left(X\right)}, \text{ and} \nonumber\\
\intinfinity \left(1-{\hat F_n}^+\left(x\right)\right)  dx =\intinfinity \int_x^\infty d \hat{F}_n\left(t\right) dx.\label{eq:2}
\end{align}
Since ${\hat F_n}^+\left(x\right)$ has bounded support on $\mathbb{R}$ $\forall n$, the integral in \eqref{eq:2} is finite.
Applying Fubini's theorem to the RHS of \eqref{eq:2}, we obtain
\begin{align*}
\intinfinity \int_x^\infty d \hat{F}_n\left(t\right) dx = \intinfinity \int_0^t dx d \hat{F}_n\left(t\right) \\
= \intinfinity t d\hat{F}_n\left(t\right) = \frac{1}{n} \sum_{i=1}^n u^+\left(X_{[i]}\right),
 \end{align*}
 where $u^+\left(X_{[i]}\right), i=1,\ldots,n$ denote the order statistics, i.e., $u^+\left(X_{[1]}\right) \le \ldots \le u^+\left(X_{[n]}\right)$.
 
Notice that 
\begin{align*}
\frac{1}{n}
\sum_{i=1}^n u^+\left(X_{[i]}\right)
=
\frac{1}{n}
\sum_{i=1}^n u^+\left(X_{i}\right)
\overset{a.s}\longrightarrow 
\EE{u^+\left(X\right)},
\end{align*}
From the foregoing,
\begin{align*}
\lim_{n\rightarrow \infty} \intinfinity \!L\left(1\!-\!\hat{F}_n\left(x\right)\right) dx
\overset{a.s} \longrightarrow
\intinfinity L \left(\Prob{u^+\left(X\right)\!>\!x}\right) dx.
\end{align*}
The claim in \eqref{eq:3} now follows by invoking the generalized dominated convergence theorem by setting $f_n = w^+(1-{\hat F_n}^+(x))$ and $g_n = L(1-\hat{F}_n(x))$, and noticing that $L(1-\hat{F}_n(x)) \xrightarrow{a.s.} L(\Prob{u^+(X)>x})$ uniformly $\forall x$. The latter fact is implied by the Glivenko-Cantelli theorem (cf. Chapter 2 of \cite{wasserman2006}).

Following similar arguments, it is easy to show that 
$$
\intinfinity w^-\left(1-{\hat F_n}^-(x)\right)  dx \rightarrow \intinfinity w^-\left(\Prob{u^-(X)>x}\right) dx.
$$
The final claim regarding the almost sure convergence of \\$\overline \C_n$ to $\C(X)$ now follows.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection*{Proof of Proposition \ref{prop:lipschitz}: Sample complexity}
%
%\begin{proof}
%Since $u^+(X)$ is bounded above by $M$ and $w^+$ is Lipschitz with constant $L$, we have
%\begin{align*}
%&\left|\intinfinity w^+\left(\Prob{u^+(X)>x}\right) dx- \intinfinity w^+\left(1- \hat F_n^+(x)\right) dx\right|
%\\
%&=  \left|\int_0^M w^+\left(\Prob{u^+(X)>x}\right) - \int_0^M w^+(1- {\hat F_n}^+(x)) dx\right|
%\\
%&\leq
%\left|\int_0^M L\cdot |\Prob{u^+(X)<x}-{\hat F_n}^+(x)| dx\right|\\
%&\leq
%LM\sup_{x\in \mathbb{R}}\left|\Prob{u^+(X)<x}-{\hat F_n}^+(x)\right|.
%\end{align*}
%Now, plugging in the DKW inequality, we obtain
%\begin{align}
%&
%\mathbb{P}\left(\left|\intinfinity w^+\left(\Prob{u^+(X)>x}\right) dx\right.\right.\nonumber\\
%&\quad\left.\left.- \intinfinity w^+\left(1- {\hat F_n}^+(x)\right) dx\right|>\epsilon/2\right)
%\nonumber\\
%&
%\leq
 %\Prob{LM\sup_{x\in \mathbb{R}} \left|(\Prob{u^+(X)<x}-{\hat F_n}^+(x)\right|>\epsilon/2} \nonumber\\
%&\leq 2 e^{-n \frac{\epsilon^2}{2 L^2 M^2}}.\label{eq:dkw1}
%\end{align}
%
%Along similar lines, we obtain
%\begin{align}
%&
%\mathbb{P}\left(\left|\intinfinity w^-\left(\Prob{u^-(X)>x}\right) dx \right.\right.\nonumber\\
%&\quad\left.\left.- \intinfinity w^-\left(1- {\hat F_n}^-(x)\right) dx\right|>\epsilon/2\right)\nonumber\\
%& \leq 2 e^{-n \frac{\epsilon^2}{2 L^2 M^2}}.\label{eq:dkw2}
%\end{align}
%
%Combining \eqref{eq:dkw1} and \eqref{eq:dkw2}, we obtain
%\begin{align*}
%&\Prob{|\overline \C_n - \C(X)|>\epsilon}\\ 
%&\le \mathbb{P}\left(\left|\intinfinity w^+\left(\Prob{u^+(X)>x}\right) dx\right.\right.\\
%&\quad\left.\left.- \intinfinity w^+\left(1- {\hat F_n}^+(x)\right) dx\right|>\epsilon/2\right) \\
%&+ 
%\mathbb{P}\left(\left|\intinfinity w^-\left(\Prob{u^-(X)>x}\right) dx\right.\right.\\
%&\left.\left.\quad- \intinfinity w^-\left(1- {\hat F_n}^-(x)\right) dx\right|>\epsilon/2\right)\\
%&\le 4 e^{-n \frac{\epsilon^2}{2 L^2 M^2}}.
%\end{align*} 
%And the claim follows. 
%\end{proof}
%
%%%%%%%%%%%%%%%%%%%%%%Lipschitz-ends-here %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Proofs for discrete valued $X$}
\label{sec:proofs-discrete}
\input{proofs-discrete}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proofs for CPT-value optimization}
\subsubsection{Proofs for CPT-SPSA}
\label{appendix:1spsa}

To prove the main result in Theorem \ref{thm:1spsa-conv}, we first show, in the following lemma, that the gradient estimate using SPSA is only an order $O(\delta_n^2)$ term away from the true gradient. The proof differs from the corresponding claim for regular SPSA (see Lemma 1 in \cite{spall}) since we have a non-zero bias in the function evaluations, while the regular SPSA assumes the noise is zero-mean. Following this lemma, we complete the proof of Theorem \ref{thm:1spsa-conv} by invoking the well-known Kushner-Clark lemma \cite{kushner-clark}.

% \todop{The bias control is with high prob and so probably all conv claims shoud be with high prob. Probably there is a simpler way out, but i dont know (yet)}
\begin{lemma}
\label{lemma:1spsa-bias}
Let $\F_n = \sigma(\theta_m,m\le n)$, $n\ge 1$.
Then, for any $i = 1,\ldots,d$, we have almost surely,  
\begin{align*}
\left| \E\left[\left.\dfrac{\overline \C_n^{\theta_n+\delta_n \Delta_n} -\overline \C_n^{\theta_n-\delta_n \Delta_n}}{2 \delta_n \Delta_n^{i}}\right| \F_n \right] - \nabla_i \C(X^{\theta_n}) \right| \xrightarrow{n \rightarrow\infty} 0.
\end{align*} 
\end{lemma}
\begin{proof}
Recall that the CPT-value estimation scheme is biased, i.e., providing samples with policy $\theta$, we obtain its CPT-value estimate as $V^{\theta}(x_0) + \epsilon^\theta$. Here $\epsilon^\theta$ denotes the bias. 
% However, since the number of samples $m_n$ go to infinity, we have $$\epsilon^{\theta_n}  \rightarrow 0 \text{ as } n \rightarrow \infty.$$

Notice that
\begin{align}
&\E\left[\dfrac{\overline \C_n^{\theta_n+\delta_n \Delta_n} -\overline \C_n^{\theta_n-\delta_n \Delta_n}}{2 \delta_n \Delta_n^{i}} \left.\right| \F_n\right] \\
&= \E\left[\dfrac{\C(X^{\theta_n + \delta_n \Delta_n}) - \C(X^{\theta_n - \delta_n \Delta_n})}{2 \delta_n \Delta_n^{i}} \left.\right| \F_n\right] + \E\left[ \eta_n \mid \F_n\right],
\end{align}
where $\eta_n = \left(\dfrac{\epsilon^{\theta_n +\delta_n\Delta} - \epsilon^{\theta_n-\delta_n\Delta}}{2\delta_n\Delta_n^{i}}\right)$
 is the bias arising out of the empirical distribution based CPT-value estimation scheme.
From Corollary \ref{cor:holder-dkw} and the fact that $\frac{1}{m_n^{\alpha/2} \delta_n} \rightarrow 0$ by assumption (A3), we have that
$$\E \eta_n \rightarrow 0 \text{ a.s. as } n \rightarrow \infty.$$ 
Thus,
\begin{align}
&\E\left[\dfrac{\overline \C_n^{\theta_n+\delta_n \Delta_n} -\overline \C_n^{\theta_n-\delta_n \Delta_n}}{2 \delta_n \Delta_n^{i}} \left.\right| \F_n\right] \nonumber\\
&\xrightarrow{n\rightarrow\infty}  \E\left[\dfrac{\C(X^{\theta_n + \delta_n \Delta_n}) -\C(X^{\theta_n - \delta_n \Delta_n})}{2 \delta_n \Delta_n^{i}} \left.\right| \F_n\right].  \label{eq:l1}
\end{align}
We now analyse the RHS of \eqref{eq:l1}.
By using suitable Taylor's expansions,
\begin{align*}
&\C(X^{\theta_n \pm \delta_n \Delta_n}) = \C(X^{\theta_n}) \pm \delta_n \Delta_n\tr \nabla \C(X^{\theta_n}) \\
&+ \frac{\delta^2}{2} \Delta_n\tr \nabla^2 \C(X^{\theta_n})\Delta_n +  \frac{\delta_n^3}{6} \nabla^3 \C(X^{\tilde\theta^{\pm}_n})(\Delta_n \otimes \Delta_n \otimes \Delta_n),
\end{align*}
where $\otimes$ denotes the Kronecker product and $\tilde \theta_n^+$ (resp. $\tilde \theta_n^-$) lie on the line segment connecting $\theta_n$ and $(\theta_n + \delta_n \Delta_n)$ (resp. $(\theta_n - \delta_n \Delta_n)$). Using (A4) and arguments similar to those used in the proof of Lemma 1 in \cite{spall}, the fourth order term in each of the Taylor's expansions above can be shown to be $O(\delta_n^3)$.  

From the above, it is easy to see that 
\begin{align*}
&\dfrac{\C(X^{\theta_n + \delta_n \Delta_n}) - \C(X^{\theta_n - \delta_n \Delta_n})}{2 \delta_n \Delta_n^i}
- \nabla_i \C(X^{\theta_n})\\
&=\underbrace{\sum_{j=1,j\not=i}^{N} \frac{\Delta_n^j}{\Delta_n^i}\nabla_j \C(X^{\theta_n})}_{(I)} + O(\delta_n^2).
\end{align*}
Taking conditional expectation on both sides, we obtain
\begin{align}
&\E\left[\dfrac{\C(X^{\theta_n + \delta_n \Delta_n}) - \C(X^{\theta_n - \delta_n \Delta_n})}{2 \delta_n \Delta_n^i} \left.\right| \F_n\right] \nonumber\\
&= \nabla_i \C(X^{\theta_n}) + \E\left[\sum_{j=1,j\not=i}^{N} \frac{\Delta_n^j}{\Delta_n^i}\right]\nabla_j \C(X^{\theta_n}) + O(\delta_n^2)\nonumber\\
&=  \nabla_i \C(X^{\theta_n}) + O(\delta_n^2).\label{eq:l2}
\end{align}
The first equality above follows from the fact that $\Delta_n$ is distributed according to a $d$-dimensional vector of Rademacher random variables and is independent of $\F_n$. The second inequality follows by observing that $\Delta_n^i$ is independent of $\Delta_n^j$, for any $i,j =1,\ldots,d$, $j\ne i$. 

The claim follows by using the fact that $\delta_n \rightarrow 0$ as $n\rightarrow \infty$.
\end{proof}

\subsection*{Proof of Theorem \ref{thm:1spsa-conv}}

\begin{proof}

%Recall that $\F_n = \sigma(\theta_m,m\le n;\Delta_m,m<n)$, $n\ge 1$.
We first rewrite the update rule \eqref{eq:theta-update} as follows: For $i=1,\ldots,d$,
\begin{align}
\theta^{i}_{n+1}  =  \Pi_i\left(\theta^{i}_n +  \gamma_n(\nabla_{i} \C(X^{\theta_n}) + \beta_n + \xi_n)\right), 
\label{eq:1spsa-equiv}
\end{align}
where 
\begin{align*}
\beta_n = &\E\left(\dfrac{(\overline \C_n^{\theta_n + \delta_n \Delta_n} -\overline \C_n^{\theta_n - \delta_n \Delta_n})}{2 \delta_n \Delta_n^{i}} \mid \F_n \right) - \nabla_i \C(X^{\theta_n}),\\
\xi_n = & \left(\dfrac{\overline \C_n^{\theta_n + \delta_n \Delta_n} -\overline \C_n^{\theta_n - \delta_n \Delta_n}}{2 \delta_n\Delta_n^{i}}\right) \\
&\qquad - \E\left(\dfrac{(\overline \C_n^{\theta_n + \delta_n \Delta_n} -\overline \C_n^{\theta_n - \delta_n \Delta_n})}{2 \delta_n \Delta_n^{i}}\mid \F_n \right).
\end{align*}
In the above, $\beta_n$ is the bias in the gradient estimate due to SPSA and $\xi_n$ is a martingale difference sequence.
% , while  Lemma \ref{lemma:1spsa-bias} and the fact that $\delta_n \rightarrow 0$ imply that $\beta_n$ vanishes asymptotically. 

Convergence of \eqref{eq:1spsa-equiv} can be inferred from Theorem 5.3.1 on pp. 191-196 of \cite{kushner-clark}, provided we verify the necessary assumptions given as (B1)-(B5) below:
\begin{enumerate}[\bfseries (B1)]
\item $\nabla \C(X^{\theta})$ is a continuous $\R^{d}$-valued function.
\item  The sequence $\beta_n,n\geq 0$ is a bounded random sequence with
$\beta_n \rightarrow 0$ almost surely as $n\rightarrow \infty$.
\item The step-sizes $\gamma_n,n\geq 0$ satisfy
$  \gamma_n\rightarrow 0 \mbox{ as }n\rightarrow\infty \text{ and } \sum_n \gamma_n=\infty.$
\item $\{\xi_n, n\ge 0\}$ is a sequence such that for any $\epsilon>0$,
\[ \lim_{n\rightarrow\infty} P\left( \sup_{m\geq n}  \left\|
\sum_{k=n}^{m} \gamma_k \xi_k\right\| \geq \epsilon \right) = 0. \]
\item There exists a compact subset $K$ which is the set of asymptotically stable equilibrium points for the following ODE:
\begin{align}
\dot\theta^{i}_t = \check\Pi_{i}\left(- \nabla \C(X^{\theta^{i}_t})\right), \text{ for } i=1,\dots,d,\label{eq:pi-ode}
\end{align}
\end{enumerate} 

In the following, we verify the above assumptions for the recursion \eqref{eq:theta-update}:\\
\begin{inparaenum}[$\bullet$]
\item (B1) holds by assumption in our setting.\\
\item Lemma \ref{lemma:1spsa-bias} above establishes that the bias $\beta_n$ is almost surely bounded and since $\delta_n \rightarrow 0$ as $n\rightarrow \infty$, it is easy to see that (B2) is satisfied for $\beta_n$. \\
\item (B3) holds by assumption (A3).\\
\item We verify (B4) using arguments similar to those used in \cite{spall} for the classic SPSA algorithm:\\
We first recall Doob's martingale inequality (see (2.1.7) on pp. 27 of \cite{kushner-clark}):
\begin{align}
\Prob{ \sup_{l\geq 0}   \left\|W_l\right\| \geq \epsilon } \le \dfrac{1}{\epsilon^2} \lim_{l\rightarrow \infty} \E \left\|W_l\right\|^2. 
\end{align}
Applying the above inequality to the martingale difference sequence $\{W_l\}$, where  $W_l := \sum_{n=0}^{l-1} \gamma_n \xi_n$, $l\ge 1$, we obtain
\begin{align}
&\Prob{ \sup_{l\geq k}   \left\|\sum_{n=k}^{l} \gamma_n \xi_n\right\| \geq \epsilon } \le \dfrac{1}{\epsilon^2} \E \left\|
\sum_{n=k}^{\infty} \gamma_n \xi_n\right\|^2 \nonumber\\
&= \dfrac{1}{\epsilon^2} \sum_{n=k}^{\infty} \gamma_n^2 \E\left\| \eta_n\right\|^2. \label{eq:b4}
\end{align}
The last equality above follows by observing that, for $m < n$, $\E(\xi_m \xi_n) = \E(\xi_m \E(\xi_n\mid \F_n))=0$.
We now bound $\E\left\| \xi_n\right\|^2$ as follows:
\begin{align}
&\E\left\| \xi_n\right\|^2\le \E\left(\dfrac{\overline \C_n^{\theta_n + \delta_n \Delta_n} -\overline \C_n^{\theta_n - \delta_n \Delta_n}}{2 \delta_n \Delta_n^i}\right)^2 \label{eq:mi}\\
\le &\left(\left(\E\left(\dfrac{\overline \C_n^{\theta_n + \delta_n \Delta_n}}{2 \delta_n \Delta_n^i}\right)^2\right)^{1/2}\right.\nonumber\\ 
&\qquad\left.+ \left(\E\left(\dfrac{\overline \C_n^{\theta_n - \delta_n \Delta_n}}{2 \delta_n \Delta_n^i}\right)^2\right)^{1/2}\right)^2 \label{eq:minko}\\
\le &\frac1{4\delta_n^2} \left[ \E\left(\frac1{(\Delta_n^i)^{2+2\alpha_1}}\right) \right]^{\frac{1}{1+\alpha1}} \nonumber\\
& \times \left(\left[\E\left[ (\overline \C_n^{\theta_n + \delta_n \Delta_n})\right]^{2+2\alpha_2}\right]^{\frac{1}{1+\alpha_2}} \right.\nonumber\\
&\left.\qquad+
\left[\E\left[ (\overline \C_n^{\theta_n - \delta_n \Delta_n})\right]^{2+2\alpha_2}\right]^{\frac{1}{1+\alpha_2}}\right)\label{eq:holder}\\
\le &\frac1{4\delta_n^2} \left(\left[\E\left[ (\overline \C_n^{\theta_n + \delta_n \Delta_n})\right]^{2+2\alpha_2}\right]^{\frac{1}{1+\alpha_2}} \right.\nonumber\\
&\left.\qquad  +
\left[\E\left[ (\overline \C_n^{\theta_n - \delta_n \Delta_n})\right]^{2+2\alpha_2}\right]^{\frac{1}{1+\alpha_2}}\right) \label{eq:h2}\\
\le & \frac{C}{\delta_n^2}, \text{ for some } C< \infty. \label{eq:h3}
\end{align}
The inequality in \eqref{eq:mi} uses the fact that, for any random variable $X$, $\E\left\|X -  E[X\mid\F_n]\right\|^2 \le \E X^2$. The inequality in \eqref{eq:minko} follows by the fact that $\E (X+Y)^2 \le \left( (\E X^2)^{1/2} + (\E Y^2)^{1/2}\right)^2$.
The inequality in \eqref{eq:holder} uses \holder's inequality, with $\alpha_1, \alpha_2>0$ satisfying $\frac{1}{1+\alpha_1} + \frac{1}{1+\alpha_2}=1$. 
The equality in \eqref{eq:h2} above follows owing to the fact that $\E\left(\frac1{(\Delta_n^i)^{2+2\alpha_1}}\right)  = 1$ as $\Delta_n^i$ is Rademacher. 
The inequality in \eqref{eq:h3} follows by using the fact that
%, for any $\theta$, the CPT-value estimate $\widehat \C(D^\theta) = \C(D^\theta) + \epsilon^\theta$. We assume a finite state-action spaced SSP (which implies that the costs $\max_{s,a} g(s,a) < \infty$) and consider only \textit{proper} policies (which implies that the total cost $D^\theta$ is bounded for any policy $\theta$) and finally, by (A1), the weight functions are Lipschitz - these together imply 
$\C(X^\theta)$ is bounded for any parameter $\theta$ and the bias $\epsilon^\theta$ is bounded by Corollary \ref{cor:holder-dkw}.
Thus, $\E\left\| \xi_n\right\|^2 \le \frac{C}{\delta_n^2}$ for some $C<\infty$. Plugging this in \eqref{eq:b4}, we obtain
\begin{align*}
 \lim_{k\rightarrow\infty} P\left( \sup_{l\geq k}   \left\|\sum_{n=k}^{l} \gamma_n \xi_n\right\|\geq \epsilon \right) \le \dfrac{d C}{\epsilon^2} \lim_{k\rightarrow\infty} \sum_{n=k}^{\infty}  \frac{\gamma_n^2}{\delta_n^2} =0.
\end{align*}
The equality above follows from (A3).\\
\item To verify (B5), observe that $\C(X^{\theta})$ serves as a strict Lyapunov function for the ODE \eqref{eq:pi-ode}. This can be seen as follows:
$$ \dfrac{d \C(X^{\theta})}{dt} = \nabla \C(X^{\theta}) \dot \theta = \nabla \C(X^{\theta}) \check\Pi \left(-\nabla \C(X^{\theta}\right) \le 0,$$
with strict inequality outside the set $\K = \{\theta \mid \check\Pi_{i} \left(-\nabla \C(X^{\theta})\right)=0, \forall i=1,\ldots,d\}$. Hence, the set $\K$ serves as the asymptotically stable attractor for the ODE \eqref{eq:pi-ode}.
\end{inparaenum} 
The claim follows from the Kushner-Clark lemma.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% \subsubsection{Proofs for CPT-SPSA-N}
% \label{sec:proofs-spsa-n}
% To simplify notation, we will use $X^{+}$ (resp. $X^{-}$) to denote $X^{\theta_n+\delta_n(\Delta_n+\widehat\Delta_n)}$ (resp. $X^{\theta_n-\delta_n(\Delta_n+\widehat\Delta_n)}$) in the proofs below.
% 
% Before proving Theorem \ref{thm:2spsa}, we bound the bias in the SPSA based estimate of the Hessian in the following lemma.
% \begin{lemma}
% \label{lemma:2spsa-bias}
% For any $i, j= 1,\ldots,d$, we have almost surely,  
% \begin{align*}
% &    \left| \E\left[\left.\dfrac{\overline \C_n^{\theta_n+\delta_n(\Delta_n+\widehat\Delta_n)} + \overline \C_n^{\theta_n-\delta_n(\Delta_n+\widehat\Delta_n)} - 2 \overline \C_n^{\theta_n}}{\delta_n^2 \Delta_n^{i}\widehat\Delta_n^j}\right| \F_n \right] \right.\\
% 		&\left.\qquad- \nabla^2_{i,j} \C(X^{\theta_n}) \right| \xrightarrow{n \rightarrow\infty} 0.
% \end{align*} 
% \end{lemma}
% \begin{proof}
% As in the proof of Lemma \ref{lemma:1spsa-bias}, we can ignore the bias from the CPT-value estimation scheme and conclude that
% \begin{align}
%     &\E\left[\dfrac{\overline \C_n^{\theta_n+\delta_n(\Delta_n+\widehat\Delta_n)} + \overline \C_n^{\theta_n-\delta_n(\Delta_n+\widehat\Delta_n)} - 2\overline \C_n^{\theta_n}}{\delta_n^2 \Delta_n^i\widehat\Delta_n^j} \left.\right| \F_n\right]  \nonumber\\
%      & \xrightarrow{n\rightarrow\infty} \E\left[\dfrac{\C(X^{+}) + \C(X^{-}) - 2\C(X^{\theta_n})}{\delta_n^2 \Delta_n^i\widehat\Delta_n^j} \left.\right| \F_n\right].  \label{eq:l21}
% \end{align}
% Now, the RHS of \eqref{eq:l21} approximates the true gradient with only an $O(\delta_n^2)$ error; this can be inferred using arguments similar to those used in the proof of Proposition 4.2 of \cite{bhatnagar2015simultaneous}. We provide the proof here for the sake of completeness.
% Using Taylor's expansion as in Lemma \ref{lemma:1spsa-bias}, we obtain
% \begin{align*}
% &\dfrac{\C(X^{+}) + \C(X^{-}) - 2\C(X^{\theta_n})}{\delta_n^2 \Delta_n^i\widehat\Delta_n^j}\\
% &=  \frac{(\Delta_n+\hat{\Delta_n})\tr \nabla^2 \C(X^{\theta_n})(\Delta_n
% +\hat{\Delta_n})}{\triangle_i(n)\hat{\triangle}_j(n)}
% + O(\delta_n^2) \\
% &= \sum_{l=1}^{d}\sum_{m=1}^{d} \frac{\Delta_n^l\nabla^2_{l,m}\C(X^{\theta_n})\Delta_n^m}{\Delta_n^i
% \hat{\Delta}_n^j}\\
% & + 2\sum_{l=1}^{d}\sum_{m=1}^{d} \frac{\Delta_n^l\nabla^2_{l,m}
% \C(X^{\theta_n})\hat{\Delta}_n^m}{\Delta_n^i
% \hat{\Delta}_n^j}\\
% &+ \sum_{l=1}^{d}\sum_{m=1}^{d} \frac{\hat{\Delta}_n^l\nabla^2_{l,m}\C(X^{\theta_n})\hat{\Delta}_n^m}{\Delta_n^i
% \hat{\Delta}_n^j} + O(\delta_n^2).
% \end{align*}
% Taking conditional expectation, we observe that the first and last term above become zero, while the second term becomes $\nabla^2_{ij}
% \C(X^{\theta_n})$. The claim follows by using the fact that $\delta_n \rightarrow 0$ as $n\rightarrow \infty$.
% \end{proof}
% 
% \begin{lemma}
% \label{lemma:2spsa-grad}
% For any $i = 1,\ldots,d$, we have almost surely,  
% \begin{align*}
% \left| \E\left[\left.\dfrac{\overline \C_n^{\theta_n + \delta_n (\Delta_n + \hat \Delta_n)} -\overline \C_n^{\theta_n - \delta_n (\Delta_n + \hat \Delta_n)}}{2 \delta_n \Delta_n^i}\right| \F_n \right] - \nabla_i \C(X^{\theta_n}) \right|\\
%  \rightarrow 0 \text{ as } n\rightarrow\infty.
% \end{align*} 
% \end{lemma}
% \begin{proof}
% %As in the proof of Lemma \ref{lemma:1spsa-bias}, we can ignore the bias from the CPT-value estimation scheme and conclude that
% %\begin{align*}
% %\quad&\E\left[\dfrac{\overline \C_n^{\theta_n + \delta_n (\Delta_n+\widehat\Delta_n)} -\overline \C_n^{\theta_n - \delta_n (\Delta_n+\widehat\Delta_n)}}{2 \delta_n \Delta_n^i} \left.\right| \F_n\right] 
% %\xrightarrow{n\rightarrow\infty}  \E\left[\dfrac{\C(X^{\theta_n + \delta_n \Delta_n}) -\C(X^{\theta_n - \delta_n \Delta_n})}{2 \delta_n \Delta_n^i} \left.\right| \F_n\right].  
% %\end{align*}
% %The rest of the proof amounts to showing that the RHS of the above approximates the true gradient with an $O(\delta_n^2)$ correcting term; this can be done in a similar manner as the proof of Lemma \ref{lemma:1spsa-bias}.
% Follows by using completely parallel arguments to that in Lemma \ref{lemma:1spsa-bias}.
% \end{proof}
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The following lemma establishes that the Hessian recursion \eqref{eq:2spsa-H} converges to the true Hessian, for any policy $\theta$.
% 
% \begin{lemma}
% \label{lemma:h-est}
% For any $i, j= 1,\ldots,d$, we have almost surely,  
% \begin{align*}
% &\left \| H^{i, j}_n - \nabla^2_{i,j} \C(X^{\theta_n}) \right \| \rightarrow 0,
% \text{ and }\\
% &\left \| \Upsilon(\overline H_n)^{-1} - \Upsilon(\nabla^2_{i,j} \C(X^{\theta_n}))^{-1} \right \| \rightarrow 0.
% \end{align*}
% \end{lemma}
% \begin{proof}
%  Follows in a similar manner as in the proofs of Lemmas 7.10 and 7.11 of \cite{Bhatnagar13SR}.
% \end{proof}
% 
% \begin{proof}\textbf{\textit{(Theorem \ref{thm:2spsa})}}
% The proof follows in a similar manner as the proof of Theorem 7.1 in \cite{Bhatnagar13SR}; we provide a sketch below for the sake of completeness.
% 
% We first rewrite the recursion \eqref{eq:2spsa} as follows:
% For $i=1,\ldots, d$
% \begin{align}
%  &\theta^{i}_{n+1} = \Pi_{i}\bigg(\theta^{i}_n + \gamma_n \sum_{j=1}^{d} \bar M^{i,j}(\theta_n) \nabla_{j} \C(X^\theta_n) + \gamma_n \zeta_n \nonumber\\
% &\qquad\qquad\qquad + \chi_{n+1} - \chi_n \bigg), \label{eq:pi-n}
% \end{align}
% where 
% \begin{align*}
% &\bar M^{i,j}(\theta) = \Upsilon(\nabla^2 \C(X^{\theta}))^{-1},\\
% & \chi_n = \sum_{m=0}^{n-1} \gamma_m \sum_{k=1}^{d} {\bar{M}}_{i,k}(\theta_m)\Bigg(
% \frac{\C(X^{-}) -
% \C(X^{+})}{2\delta_m \Delta^k_m} 
%  \\
%  &- E\left[\frac{\C(X^{-}) -
% \C(X^{+})}{2\delta_m \Delta^k_m} 
% \mid {\cal F}_m\right]\Bigg) \text{ and}\\
% &\zeta_n \!=\! \E\!\left[\left.\dfrac{\overline \C_n^{\theta_n + \delta_n (\Delta_n + \hat \Delta_n)} \!-\!\overline \C_n^{\theta_n - \delta_n (\Delta_n + \hat \Delta_n)}}{2 \delta_n \Delta_n^i}\right| \F_n \right] \!-\! \nabla_i \C(X^{\theta_n}).
% \end{align*}
% In lieu of Lemmas \ref{lemma:2spsa-bias}--\ref{lemma:h-est}, it is easy to conclude that $\zeta_n \rightarrow 0$ as $n\rightarrow \infty$, $\chi_n$ is a martingale difference sequence and that $\chi_{n+1} - \chi_n \rightarrow 0$ as $n\rightarrow \infty$. 
% Thus, it is easy to see that \eqref{eq:pi-n} is a discretization of the ODE:
% \begin{align}
% \dot\theta^{i}_t = \check\Pi_{i}\left(- \nabla \C(X^{\theta^{i}_t}) \Upsilon(\nabla^2 \C(X^{\theta_t}))^{-1} \nabla \C(X^{\theta^{i}_t}) \right).
% \label{eq:n-ode}
% \end{align}
% Since $\C(X^{\theta})$ serves as a Lyapunov function for the ODE \eqref{eq:n-ode}, it is easy to see that the set \\$\K \!\!=\! \{\theta \left|\right.
% \nabla \C(X^{\theta^{i}})  \check\Pi_{i}\left(-\Upsilon(\nabla^2 \C(X^{\theta}))^{-1} \nabla \C(X^{\theta^{i}})\right)\!=\!0$,$ \forall i=1,\ldots,d\}$ is an asymptotically stable attractor set for the ODE \eqref{eq:n-ode}. The claim now follows from Kushner-Clark lemma.
% \end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Proofs for CPT-MPS}
\label{appendix:mras}

%\textbf{\textit{Natural exponential family (NEF).}} A parameterized family 
%$\left\{f(\cdot,\theta), \theta \in \varTheta \subseteq \Re^m \right\}$
%on $\mathcal{X}$
%is said to to NEF
%if there exist $h:\Re^n\rightarrow \Re$,
%$\Upsilon:\Re^n \rightarrow \Re^m$,
%and $K:\Re^m\rightarrow \Re$ such that
%%
%\begin{equation}\label{eqn:nef}
%f(\mathbf{x},\theta)=\exp\left\{\theta^{T}\Upsilon(\mathbf{x})-K(\theta) \right\}h(\mathbf{x}),~~\forall\, \theta \in \varTheta,
%\end{equation}
%%
%where $K(\theta)=\ln \int_{\mathbf{x}\in \mathcal{X}}\exp\left\{\theta^{T}\Upsilon(\mathbf{x})\right\}h(\mathbf{x})\nu(d\mathbf{x})$, $\varTheta$ is the natural parameter space $\varTheta=\{\theta\in\Re^m:|K(\theta)|<\infty \},$
%and the superscript ``$T$'' denotes the vector transposition.


Since we obtain samples of the objective (CPT) in a manner that differs from MRAS$_2$, we need to establish that the thresholding step in Algorithm \ref{alg:mras} achieves the same effect as it did in MRAS$_2$. This is achieved by the following lemma, which is a variant of Lemma 4.13 from \cite{chang2013simulation}, adapted to our setting. 

\begin{lemma}\label{lemma:iofo}
    The sequence of random variables $\{\theta^*_n,n=0,1,\ldots \}$ in Algorithm \ref{alg:mras} converges w.p.1 as $n\rightarrow \infty$.
\end{lemma}
\begin{proof}
    Let $\mathcal{A}_n$ be the event that either the first if statement (see \ref{step:3a}) is true or the second if statement in the else clause (see \ref{step:3b}) is true within the \textit{thresholding} step of Algorithm \ref{alg:mras}. Let $\mathcal{B}_n:=\{\C(X^{\theta^*_{n}})-\C(X^{\theta^*_{n-1}})\leq \frac{\varepsilon}{2}\}$.
    Whenever $\mathcal{A}_n$ holds, we have $\overline \C_n^{\theta^*_{n}}-\overline \C_n^{\theta^*_{n-1}}\geq \varepsilon$ and hence, we obtain

\begin{align*}
&P(\mathcal{A}_{n}\cap \mathcal{B}_n) \\
&\leq P 
\left(\big\{\overline \C_n^{\theta^*_{n}}-\overline \C_{n-1}^{\theta^*_{n-1}}\geq \varepsilon\big\}\right. \\
&\quad\left.\cap \left\{\C(X^{\theta^*_{n}})-\C(X^{\theta^*_{n-1}})\leq \frac{\varepsilon}{2}\right\}\right)\\
 &\leq 
P\Big(\bigcup_{\theta\in\Lambda_n, \theta'\in \Lambda_{n-1}} \Big\{\big\{\overline \C_n^{\theta}-\overline \C_{n-1}^{\theta'}\geq \varepsilon\big\} \\
&\qquad\quad\cap \left\{\C(X^{\theta})-\C(X^{\theta'})\leq \frac{\varepsilon}{2}\right\} \Big\}\Big)\\
&\leq  \sum_{{\theta\in \Lambda_n},{\theta'\in \Lambda_{k-1}}}P\left(\big\{\overline \C_n^{\theta}-\overline \C_{n-1}^{\theta'}\geq \varepsilon\big\} \right.\\
&\qquad\quad \left.\cap
\left\{\C(X^{\theta})-\C(X^{\theta'})\leq \frac{\varepsilon}{2}\right\}\right) \\
&\leq  |\Lambda_n||\Lambda_{n-1}|\sup_{\theta,\theta'\in \Theta}P\left(\big\{\overline \C_n^{\theta}-\overline \C_{n-1}^{\theta'}\geq \varepsilon\big\} \right.\\
&\quad\qquad \left.\cap \left\{\C(X^{\theta})-\C(X^{\theta'})\leq \frac{\varepsilon}{2}\right\} \right) \\
&\leq |\Lambda_n||\Lambda_{n-1}|\sup_{\theta,\theta'\in \Theta}P\left(\overline \C_n^{\theta}-\overline \C_{n-1}^{\theta'}-\C(X^{\theta})+\C(X^{\theta'})\geq \frac{\varepsilon}{2}\right) \\
&\leq |\Lambda_n||\Lambda_{n-1}|\sup_{\theta,\theta'\in \Theta}\left(P\left(\overline \C_n^{\theta}-\C(X^{\theta})\geq \frac{\varepsilon}{4}\right)\right. \\
&\qquad\left. + P\left(\overline \C_{n-1}^{\theta'}-\C(X^{\theta'})\geq \frac{\varepsilon}{4}\right)\right) \\
&\leq  4|\Lambda_n||\Lambda_{k-1}|
 e^{- \frac{m_n\epsilon^2}{8 L^2 M^2}}.
\end{align*}
From the foregoing, we have $\sum_{n=1}^{\infty}P\left(\mathcal{A}_n \cap \mathcal{B}_n\right) < \infty$ since $m_n \rightarrow \infty$ as $n\rightarrow \infty$.  Applying the Borel-Cantelli lemma, we obtain
\begin{align*}
P\left(\mathcal{A}_n \cap \mathcal{B}_n~\mbox{i.o.} \right)=0.
\end{align*}
From the above, it is implied that if $\mathcal{A}_n$ happens infinitely often,
then $\mathcal{B}_n^c$ will also happen infinitely often. Hence,

\begin{align*}
&\sum_{n=1}^{\infty}\big[\C(X^{\theta^*_n})-\C(X^{\theta^*_{n-1}})\big] 
=\sum_{n:~\mathcal{A}_n \text{occurs}}\big[\C(X^{\theta^*_n})-\C(X^{\theta^*_{n-1}})\big]\\
&+\sum_{n:~\mathcal{A}_n^c \text{occurs}}\big[\C(X^{\theta^*_n})-\C(X^{\theta^*_{n-1}})\big] \\ 
&=\sum_{n:~\mathcal{A}_n \text{occurs}}\big[\C(X^{\theta^*_n})-\C(X^{\theta^*_{n-1}})\big]\\ 
&=\sum_{n:~\mathcal{A}_n\cap \mathcal{B}_n \text{occurs}}\big[\C(X^{\theta^*_n})-\C(X^{\theta^*_{n-1}})\big]\\
&\quad +\sum_{n:~\mathcal{A}_n\cap \mathcal{B}_n^c \text{occurs}}\big[\C(X^{\theta^*_n})-\C(X^{\theta^*_{n-1}})\big] \\ 
&=\infty \text{~~w.p.1, since $\varepsilon>0$.}
\end{align*}
In the above, the first equality follows from the fact that if the else clause in the second if statement (see \ref{step:3c}) in Algorithm \ref{alg:mras} is hit, then $\theta^*_{n} = \theta^*_{n-1}$.
%
From the last equality above, we conclude that it is a contradiction because, $\C(X^{\theta}) < \C(X^{\theta^*})$ for any $\theta$ (since $\theta^*$ is the global maximum). The main claim now follows since $\mathcal{A}_n$ can happen only a finite number of times.
\end{proof}

\subsection*{Proof of Theorem \ref{thm:mras}}
\begin{proof}
Once we have established Lemma \ref{lemma:iofo}, the rest of the proof follows in an identical fashion as the proof of Corollary 4.18 of \cite{chang2013simulation}. This is because our algorithm operates in a similar manner as MRAS$_2$ w.r.t. generating the candidate solution using a parameterized family $f(\cdot, \eta)$ and updating the distribution parameter $\eta$. The difference, as mentioned earlier, is the manner in which the samples are generated and the objective (CPT-value) function is estimated. The aforementioned lemma established that the elite sampling and thresholding achieve the same effect as that in MRAS$_2$ and hence the rest of the proof follows from \cite{chang2013simulation}.
\end{proof}
