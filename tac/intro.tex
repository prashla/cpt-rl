%!TEX root =  cpt-rl-icml.tex
% Since the beginning of its history, mankind has been deeply immersed in 
% 	designing and improving systems to serve human needs.
% Policy makers are busy with designing 
% 	systems that serve the education, transportation, economic, health and other 
% 	needs of the public,
% while private sector enterprises work hard at creating 
% 	and optimizing systems to better serve  
% 	specialized needs of their customers.
% While it has been long recognized that 
% 	understanding human behavior is a prerequisite 
% 	to best serving human needs \cite{Simon:1959kd},
% %	and the private sector has adopted this strategy a while ago \cite{}
% %	and the private sector quickly adopted this strategy,
% 	it is only recently that this approach is gaining a wider recognition.%
% \footnote{
% As evidence for this wider recognition in the public sector,
% we can mention a recent executive order of the White House
% calling for the use of behavioral science in public policy making, 
% or the establishment of the ``Committee on Traveler Behavior and Values'' in the Transportation
% Research Board in the US.}
% %Using Behavioral Science Insights to Better Serve the American People

In this paper we consider \emph{stochastic optimization problems}
where a designer optimizes the system 
to produce outcomes that are maximally aligned with the preferences of 
one or possibly multiple humans.
As a running example, consider traffic optimization where the goal is to maximize
travelers' satisfaction, a challenging problem 
%many may agree is  still inadequately addressed today, at least 
in big cities.
In this example, the outcomes (``return'') are travel times, or delays. 
To capture human preferences, the outcomes are mapped to a single numerical quantity.
%We will make the assumption that human preferences can 
%To create a single numerical quantity that faithfully captures human preferences, 
%a \emph{risk metrics}, mapping the random returns to some scalar deterministic quantity, is used.
While preferences of rational agents facing decisions with stochastic outcomes can be modeled using expected utilities,
i.e., the expectation of a nonlinear transformation, such as the exponential function, of the rewards or costs
\cite{NeuMo44,fishburn1970expectedutility}, 
	humans are subject to various emotional and cognitive biases,
	and, as the psychology literature points out, human preferences 
	are inconsistent with expected utilities regardless of what nonlinearities are used
	 \cite{allais53,ellsberg61,kahneman1979prospect}.
An approach that gained 
%Behavioral scientists use alternate approaches to model human preferences.
	strong support amongst psychologists, behavioral scientists and economists (cf. \cite{starmer2000developments,quiggin2012generalized})
	is based on \cite{kahneman1979prospect}'s celebrated \emph{prospect theory} (PT),
	the theory that we will base our models of human preferences on
	 in this work.
More precisely, we will use \emph{cumulative prospect theory} (CPT),
 	a later, refined variant of prospect theory due to \cite{tversky1992advances}, 
	which superseded prospect theory (e.g.,\cite{Barberis:2012vs}).
CPT generalizes expected utility theory in that in addition to having a utility function transforming
	the outcomes, another function is introduced which distorts the probabilities in the cumulative distribution function.
As compared to prospect theory, CPT is monotone with respect to stochastic dominance, a property
	that is thought to be useful and (mostly) consistent with human preferences.
	%\footnote{See Appendix \ref{sec:appendix-cpt-intro} for an introduction to PT/CPT and an explanation of why expected utility theory is inconsistent with human preferences.}
	
\if0	

Popular approaches that use such risk metrics include the exponential utility formulation 
(cf. \cite{borkar2010learning}) that implicitly controls the variance.
An alternative is a to consider constrained formulations 
with explicit constraints on the variance of the return (cf. \cite{tamar2012policy,Prashanth13AC}). 
Another constraint alternative is to bound a coherent risk measure such as Conditional Value-at-Risk (CVaR), 
while minimizing the usual cost objective (cf. \cite{borkar2010risk,prashanth2014policy}).  

The risk metrics underlying the above-mentioned works 
are based on the assumption that human decision makers are rational and/or consistent.
While this may hold in certain restricted settings, a large body of literature indicates that humans are neither rational,
\todoc{Add literature supporting this. At least three books:)}
nor consistent (which, in fact, is an unsurprising fact, at least in the experience of the authors of the paper).
In other words, traditional approaches are based on the belief that optimizing the expected utility (EU) is appealing for human subjects. However, there is substantial evidence that this is not case - see 
the survey article \cite{starmer2000developments} and Chapter 4 of the book \cite{quiggin2012generalized}. In particular, the aforementioned references describe the Allais and Ellsberg paradoxes popular among economists for arguing against EU. 
Thus, if the goal is to produce outcomes that are best aligned with human preferences,
an alternative approach is required.
A singularly popular and successful approach in behavioral science and economics
is based on \textit{prospect theory (PT)} \cite{kahneman1979prospect} 
and its later enhancement, the so-called \textit{cumulative prospect theory} (CPT) \cite{tversky1992advances}.
CPT is a rank dependent expected utility model \cite{quiggin2012generalized} that incorporates decision weights to distort probabilities. 
The suitability of this approach to model human decision making (and thus preferences) has been widely documented \cite{prelec1998probability}, \cite{wu1996curvature}, \cite{conlisk1989three}, \cite{camerer1989experimental}, \cite{camerer1992recent}, \cite{harless1992predictions}, \cite{sopher1993test}, \cite{camerer1994violations}, \cite{gonzalez1999shape}, \cite{abdellaoui2000parameter}.
PT/CPT has been applied in a variety of domains, for e.g., healthcare \cite{lenert1999associations},  seismic design \cite{goda2008application}, transportation \cite{gao2010adaptive},\cite{fujii2004drivers}, \cite{ramming2001network}, online auctions \cite{weinberg2005exploring}, insurance  \cite{machina1995non} and finance \cite{barberis1999prospect}, \cite{epstein1989substitution}, \cite{epstein1991substitution}.
\fi

% \tikzset{
%   pobl/.style={
%     inner sep=0pt, outer sep=0pt, fill=#1,
%   },
%   pobl gron/.style n args={2}{
%     pobl=#1, rounded corners=#2,
%   },
%   pics/person/.style n args={3}{
%     code={
%       \node (-corff) [pobl=#1, minimum width=.25*#2, minimum height=.375*#2, rotate=#3, pic actions] {};
%       \node (-pen) [minimum width=.3*#2, circle, pobl=#1, outer sep=.01*#2, anchor=south, rotate=#3, pic actions] at (-corff.north) {};
%       \node (-coes dde) [pobl gron={#1}{1pt}, anchor=north west, minimum width=.12125*#2, minimum height=.25*#2, rotate=#3, pic actions] at (-corff.south west) {};
%       \node [pobl=#1, anchor=north, minimum width=.12125*#2, minimum height=.15*#2, rotate=#3, pic actions] at (-coes dde.north) {};
%       \node (-coes chwith) [pobl gron={#1}{1pt}, anchor=north east, minimum width=.12125*#2, minimum height=.25*#2, rotate=#3, pic actions] at (-corff.south east) {};
%       \node [pobl=#1, anchor=north, minimum width=.12125*#2, minimum height=.15*#2, rotate=#3, pic actions] at (-coes chwith.north) {};
%       \node (-braich dde) [pobl gron={#1}{.75pt}, minimum width=.075*#2, minimum height=.325*#2, outer sep=.0064*#2, anchor=north west, rotate=#3, pic actions] at (-corff.north east)  {};
%       \node [pobl=#1, minimum width=.05*#2, minimum height=.2*#2, outer sep=.0064*#2, anchor=north west, rotate=#3, pic actions] at (-corff.north east) {};
%       \node (-braich chwith) [pobl gron={#1}{.75pt}, minimum width=.075*#2, minimum height=.325*#2, outer sep=.0064*#2, anchor=north east, rotate=#3, pic actions] at (-corff.north west) {};
%       \node [pobl=#1, minimum width=.0375*#2, minimum height=.2*#2, outer sep=.0064*#2, anchor=north east, rotate=#3, pic actions] at (-corff.north west) {};
%       \node (-fit person) [fit={(-pen.north) (-braich dde.east) (-coes chwith.south) (-braich chwith.west)}] {};
%       %\node (-pwy) [below=25pt of -fit person, every pin] {\tikzpictext};
%       %\draw [every pin edge] (-fit person) -- (-pwy);
%     },
%   },
% }
% \tikzstyle{block} = [draw, fill=white, rectangle,
%    minimum height=3em, minimum width=6em]
% \tikzstyle{sum} = [draw, fill=white, circle, node distance=1cm]
% \tikzstyle{input} = [coordinate]
% \tikzstyle{output} = [coordinate]
% \tikzstyle{pinstyle} = [pin edge={to-,thin,black}]
% 
% \begin{figure}[h]
% \centering
% \scalebox{0.65}{\begin{tikzpicture}
% \node [block,fill=blue!20, minimum height=4em,] (world) {\makecell{\large\bf Stochastic \\\large\bf System}}; 
% \node [block,fill=red!20, below=2.5cm of world] (agent) {\makecell{\large\bf Optimization \\\large\bf Algorithm}}; 
% \node [circle,fill=brown!50!black,right=2cm of world,label=above:{\bf Reward}] (tmp) {};
% \coordinate [below left=2.5cm of world] (tmp2);
% \coordinate [below right=1cm of agent] (belowagent);
% 
% \draw [->,thick] (world)  -| (tmp) |-   (agent);
% \draw [->,thick] (agent)  -| (tmp2) |-   (world);
% %\draw [->] (agent) --   (world);
% \draw pic (person)  [right =3cm of tmp] {person={blue}{50pt}{0}};
% \coordinate [right=2.8cm of tmp] (tmp3);
% \coordinate [right=3.3cm of tmp] (tmp31);
% \coordinate [right=1cm of tmp3] (belowhuman);
% \coordinate [right=3.5cm of tmp3] (tmp4);
% \draw [->,thick] (tmp) --   (tmp3);
% \draw [-,thick,dotted] (tmp31)  -- node[below right=0.1cm and 0.5cm] {\textbf{Preferences}} (belowhuman) --  (tmp4);
% \draw [->,thick,dotted] (tmp4)  |-  (belowagent) -|  (agent.south);
% \end{tikzpicture}}
% \caption{Operational flow of a human-based decision making system
% }
% \label{fig:flow}
% \end{figure}
% %\begin{tikzpicture}
%   %[
%     %every pin edge/.append style={latex-, shorten <=-2.5pt},
%   %]

%
%In the realm of sequential decision making under uncertainty, we propose a CPT-based risk measure.  In particular,  
%The current RL solutions cannot handle distortions and my current work is to develop both prediction and control schemes for probabilistically distorted MDPs.
%
%\todop[inline]{Add refs for distorted weights}
%In this paper, we consider a risk measure based on \textit{cumulative prospect theory} (CPT) \cite{tversky1992advances}, which is a non-coherent and non-convex measure that is well known among psychologists and economists to be a good model for human decision-making systems, with strong empirical support. In this paper, we incorporate CPT-based criteria into the classic objective \textit{value function} in a reinforcement learning framework. Intuitively this combination is appealing because it taps into the notion of how humans evaluate outcomes and also, a CPT objective leads to a randomized policy, which although harder to estimate often leads to more intuitively appealing behavior, as illustrated via an example below and also the numerical experiments later.

%%%%%%%%%%%%\
\subsection*{Our contributions\footnote{A preliminary version of this paper was published in ICML 2016 \cite{la2016cumulative}. In comparison to the conference version, this paper includes an additional optimization algorithm, formal proofs of convergence of both estimation and optimization algorithms, some additional experiments and a revised presentation.}}

To the  best of our knowledge, we are the first to combine CPT with stochastic optimization. Although on the surface the combination may seem straightforward, in fact there are many research challenges that arise from trying to optimize a CPT objective in the stochastic optimization framework, as we will soon see. 
We outline these challenges as well as our approach to addressing them below. 

The first challenge stems from the fact that the CPT-value assigned to a random variable is defined through a nonlinear transformation of the cumulative distribution functions associated with the random variable (cf. \cref{sec:cpt-val} for the definition). 
Hence, even the problem of estimating the CPT-value given a random sample is challenging.
%\textbf{\textit{Prediction:}} In the case of the classic value function, which is an expectation, a simple sample mean can be used for estimation, facilitating the use of temporal difference type algorithms. On the other hand, 
%CPT-value   involves a distribution that is distorted using non-linear weight functions and hence, 
%requires that the \textit{entire} distribution to be estimated.\\ 
%\textit{Solution:} 
In this paper, we consider a natural quantile-based estimator and analyze its behavior.
Under certain technical assumptions, we prove consistency and give sample complexity bounds, the latter based on the
 Dvoretzky-Kiefer-Wolfowitz (DKW) theorem.
As an example, we show that the sample complexity for estimating the CPT-value 
for Lipschitz probability distortion (so-called ``weight'') functions is  $O\left(\frac1{\epsilon^2}\right)$, which coincides with the canonical rate for Monte Carlo-type schemes and is thus unimprovable. Since weight-functions that fit well to human preferences are only  \holder continuous, we also consider this case and find that (unsurprisingly) the sample complexity  jumps to $O\left(\frac1{\epsilon^{2/\alpha}}\right)$ where $\alpha\in (0,1]$ is the weight function's \holder exponent.
%However, for weight functions which are only \holder continuous with exponent $\alpha \in (0,1)$,
%Thus, at least in this case, 
%At the same time, 
% while \holder continuous with constant $\alpha$ results in a rate  $O\left(\frac1{\epsilon^{2/\alpha}}\right)$, for achieving a $\epsilon$-close estimate.\\

Our results on estimating CPT-values form the basis of the algorithms that we propose to maximize CPT-values based on interacting either with a real environment, or with a simulator. 
% We set up this problem as an instance of policy search: 
We consider a smooth parameterization of the CPT-value and propose two algorithms for updating the CPT-value parameter. The first algorithm is a stochastic gradient scheme that uses two-point randomized gradient estimators, borrowed from simultaneous perturbation stochastic approximation (SPSA) \cite{spall}\footnote{A second-order CPT-value optimization scheme based on SPSA is described in \cite{la2016cumulative}.}. 
% We employ a three-point SPSA-based Hessian estimator proposed in \cite{bhatnagar2015simultaneous}. 
%\textbf{\textit{Control:}} 
%Designing algorithms in order to find a \textit{CPT-optimal} policy is challenging because CPT-value is a non-coherent and non-convex risk measure that does not lend itself to dynamic programming approaches such as value/policy iteration due to the lack of a ``Bellman equation''. 
%Thus, it is necessary to design new simulation optimization scheme that use sample CPT-value estimates to optimize the policy, which is generally \textit{randomized}. While classic simulation optimization settings usually have a zero mean noise in function evaluations, our setting one has to tradeoff simulation cost with the bias in a manner such that the resulting policy optimization scheme cancels the bias effect and converges. \\
%\textit{Solution:} 
%Using the well-known idea of simultaneous perturbation stochastic approximation (SPSA) from the \textit{simulation optimization} literature \cite{fu2015handbook}, we propose and analyze a gradient method for CPT value optimization in sequential decision making problems
%under uncertainty.
The second algorithm is a gradient-free method that is adapted from \cite{chang2013simulation}. The idea is to use a reference model that eventually concentrates on the global minimum and then empirically approximate this reference distribution well-enough. The latter is achieved via natural exponential families in conjunction with Kullback-Leibler (KL) divergence to measure the ``distance'' from the reference distribution. 
Guaranteeing convergence of the aforementioned two CPT-value optimization is challenging because only  \emph{biased} estimates of the CPT-value are available. We propose a particular way of controlling the arising bias-variance tradeoff and establish convergence for all proposed algorithms.
% We establish that our algorithm converges to a globally CPT-value optimal policy (assuming it exists).
% Here a new challenge arises, which is that we can only feed the two-point randomized gradient estimator with \emph{biased} estimates of the CPT-value. To guarantee convergence, we propose a particular way of controlling the arising bias-variance tradeoff.

%Since SPSA by default assumes that the unbiased point-estimates of the optimization objective are available, which
%is not the case in our problem, we extend the analysis of SPSA to handle the case when the optimization objective estimates are noisy, with controlled bias.
%
%We derive the condition that specifies the rate at which the number of samples for predicting the CPT-value should increase such that the bias of CPT-value estimates vanishes asymptotically (see (A3) later).
%
%, while the second is a Newton algorithm that also uses SPSA-based estimates of the gradient and also the Hessian. We remark again that, unlike traditional settings for SPSA, our estimates for CPT-value have a non-zero (albeit controlled) bias. We establish that our algorithms converge to a locally CPT-value optimal policy. 

% \todoc[inline]{Im not sure what to do with the following para, as we havent talked about RL and position our work as CPT + sto-opt}

\textit{Related work.}
Various risk measures such as have been proposed in literature - mean variance tradeoff \cite{markowitz1952portfolio}, exponential utility \cite{Arrow1971}, value at risk (VaR) and conditional value at risk (CVaR) \cite{rockafellar2000optimization} to name a few. A large body of literature involves risk-sensitive optimization in the context of MDPs.   
The stochastic optimization context of this paper translates to a risk-sensitive reinforcement learning (RL) problem and it has been in earlier works that risk-sensitive RL is generally hard to solve. 
For instance, in \cite{Sobel82VD}, \cite{filar1989variance} and \cite{mannor2013algorithmic}, the authors provide NP-hardness results for finding a globally variance-optimal policy in discounted and average reward MDPs.
Solving CVaR constrained MDPs is equally complicated (cf. \cite{borkar2010risk,prashanth2014policy,tamar2014optimizing}). 
% Finally, we point out that the CPT-value is a generalization of all the risk measures above in the sense that one can recover these particular risk measures such as VaR and CVaR by appropriate choices of the distortions used in the definition of the CPT-value.
In the context of an abstract MDP setting, a CPT-based risk measure has been proposed in \cite{lin2013stochastic}. Unlike \cite{lin2013stochastic},
\begin{inparaenum}[\it (i)]
\item we do not assume a nested structure for the CPT-value, %\eqref{eq:cpt-mdp} 
and this implies the lack of a Bellman equation for our CPT measure;
\item we do not assume model information, i.e., we operate in a more general stochastic optimization setting;
\item we develop both estimation and optimization algorithms with convergence guarantees for the CPT-value function.
\end{inparaenum}

The rest of the paper is organized as follows: 
In Section~\ref{sec:cpt-val}, we introduce the notion of CPT-value of a general random variable $X$.
In Section~\ref{sec:cpt-sampling}, we
describe the empirical distribution based scheme for estimating the CPT-value of any random variable. In Section \ref{sec:cpt-control}, we present the gradient-based algorithms for optimizing the CPT-value. 
%Next, in Section \ref{sec:mras}, we present a gradient-free model-based algorithm for CPT-value optimization in an MDP. 
We provide the proofs of convergence for all the proposed algorithms in Section~\ref{sec:convergence}.
We present the results from numerical experiments for the CPT-value estimation scheme in Section~\ref{sec:expts} and finally, provide the concluding remarks in Section~\ref{sec:conclusions}.
